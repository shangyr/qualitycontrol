Some weights of the model checkpoint at yikuan8/Clinical-Longformer were not used when initializing LongformerModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']
- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/home/llm_user/.conda/envs/torch39/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
====================Configuration====================
data_path:/home/llm_user/prompt-gnn/data/mimic-3
train_path:/home/llm_user/prompt-gnn/data/mimic-3/train.json
dev_path:/home/llm_user/prompt-gnn/data/mimic-3/dev.json
test_path:/home/llm_user/prompt-gnn/data/mimic-3/test.json
label_idx_path:/home/llm_user/prompt-gnn/data/mimic-3/label2id.txt
kg_graph_path:/home/llm_user/prompt-gnn/data/mimic-3/triples.txt
entity_path:/home/llm_user/prompt-gnn/data/mimic-3/entities.txt
emr2kg_path:/home/llm_user/prompt-gnn/data/mimic-3/emr2kg.pkl
label_name_path:/home/llm_user/prompt-gnn/data/mimic-3/label_name.txt
idf_path:/home/llm_user/prompt-gnn/data/mimic-3/idf.pkl
tf_idf_radio:0.9
entity_embedding_path:/home/llm_user/prompt-gnn/data/mimic-3/entities_embed_bert.pth
kge_trainer:BertKGETrainer
language:en
max_length:3000
label_smooth_lambda:0.0
bert_lr:2e-05
other_lr:0.001
batch_size:6
logic_node_num:8
entity_embedding_dim:100
gnn_layer:2
num_hop:2
accumulation_steps:1
path_type:v2
bert_path:yikuan8/Clinical-Longformer
config:MIMICConfig
data_version:
dataset:PromptGNNDataset
dropout_rate:0.5
embedding_dim:768
epochs:20
hidden_size:512
local_rank:None
loss_fn:BCE
model_name:PromptGATEdge
patience:5
result_path:/home/llm_user/prompt-gnn/logs/results/PromptGATEdge_seed_1_2023-08-16-21-49-18
sample_radio:2
save_model_path:/home/llm_user/prompt-gnn/logs/checkpoints/PromptGATEdge_seed_1_2023-08-16-21-49-18.pth
seed:1
test_freq:1
test_model_path:
test_only:False
trainer:PromptGNNTrainer
use_pretrain_embed_weight:True
use_wandb:False
warmup_rate:0.3
class_num:50
entity_num:0
====================Configuration====================

实体数量: 50335
二元关系数量: 3316590
二元关系平均节点度: 65.89033475712725
Loading data file...
Loading data file...
Loading data file...

=== Epoch 0 train ===
2023-08-16 21:49:51.260391

[MACRO] accuracy, precision, recall, f-measure
0.1387, 0.3491, 0.1795, 0.2371
[MICRO] accuracy, precision, recall, f-measure
0.2114, 0.6119, 0.2441, 0.3490
              precision    recall  f1-score   support

       401.9     0.6368    0.3616    0.4613       708
       38.93     0.4602    0.3994    0.4277       333
       428.0     0.7092    0.5935    0.6462       337
      427.31     0.5903    0.5202    0.5530       396
      414.01     0.7660    0.6495    0.7029       388
       96.04     0.4591    0.3202    0.3773       228
        96.6     0.6739    0.1378    0.2288       225
       584.9     0.3813    0.5171    0.4390       292
      250.00     0.6707    0.5709    0.6168       289
       96.71     0.6505    0.4261    0.5149       284
       272.4     1.0000    0.0084    0.0167       475
      518.81     0.7724    0.3740    0.5040       254
       99.04     0.0000    0.0000    0.0000        54
       39.61     0.9116    0.9423    0.9267       208
       599.0     0.0000    0.0000    0.0000       214
      530.81     0.0000    0.0000    0.0000       270
       96.72     0.5660    0.4286    0.4878       140
       272.0     0.2400    0.0397    0.0682       151
       285.9     0.0000    0.0000    0.0000       210
       88.56     0.6000    0.6940    0.6436       134
       244.9     0.0000    0.0000    0.0000       180
         486     0.4375    0.0881    0.1466       159
       38.91     1.0000    0.0155    0.0305       129
       285.1     0.8000    0.0212    0.0412       189
       36.15     0.6683    0.9928    0.7988       138
       276.2     0.0000    0.0000    0.0000       168
         496     0.0000    0.0000    0.0000       130
       99.15     1.0000    0.0364    0.0702        55
      995.92     0.6667    0.0448    0.0839       134
      V58.61     0.0000    0.0000    0.0000       176
       507.0     0.2857    0.0211    0.0392        95
       038.9     0.3333    0.0086    0.0168       116
       88.72     0.0000    0.0000    0.0000        96
       585.9     0.0000    0.0000    0.0000       150
      403.90     0.0000    0.0000    0.0000       162
         311     0.0000    0.0000    0.0000       179
       305.1     0.0000    0.0000    0.0000       167
       37.22     0.5500    0.2292    0.3235        96
         412     0.0000    0.0000    0.0000       112
       33.24     0.0000    0.0000    0.0000        96
       39.95     0.8000    0.0455    0.0860        88
       287.5     0.0000    0.0000    0.0000       138
      410.71     0.3704    0.2817    0.3200        71
       276.1     0.0000    0.0000    0.0000       129
      V45.81     0.0000    0.0000    0.0000        96
       424.0     0.0000    0.0000    0.0000        72
       45.13     0.0000    0.0000    0.0000        82
      V15.82     0.0000    0.0000    0.0000       157
       511.9     0.0000    0.0000    0.0000        85
       37.23     0.4545    0.2083    0.2857        48

   micro avg     0.6119    0.2441    0.3490      9283
   macro avg     0.3491    0.1795    0.1971      9283
weighted avg     0.4309    0.2441    0.2697      9283
 samples avg     0.4559    0.2283    0.2803      9283

[0.6268277177368087, 0.43560076287349014, 0.328099173553719]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/llm_user/.conda/envs/torch39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/llm_user/.conda/envs/torch39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

Train step of epoch 0: 100%|██████████| 1345/1345 [1:07:50<00:00,  1.94s/it, loss=0.361]
Train step of epoch 0: 100%|██████████| 1345/1345 [1:07:50<00:00,  3.03s/it, loss=0.361]

[MACRO] accuracy, precision, recall, f-measure
0.1369, 0.3417, 0.1743, 0.2309
[MICRO] accuracy, precision, recall, f-measure
0.2096, 0.6304, 0.2390, 0.3466
              precision    recall  f1-score   support

       401.9     0.6256    0.3265    0.4291       778
       38.93     0.4760    0.3955    0.4321       402
       428.0     0.7903    0.5806    0.6694       422
      427.31     0.6382    0.5255    0.5764       470
      414.01     0.7797    0.6345    0.6996       435
       96.04     0.5116    0.3777    0.4346       233
        96.6     0.6875    0.1447    0.2391       228
       584.9     0.4603    0.5608    0.5056       362
      250.00     0.6233    0.5500    0.5844       340
       96.71     0.5682    0.3876    0.4608       258
       272.4     0.5000    0.0036    0.0072       548
      518.81     0.7899    0.3686    0.5027       255
       99.04     0.0000    0.0000    0.0000        51
       39.61     0.9221    0.9425    0.9322       226
       599.0     0.0000    0.0000    0.0000       251
      530.81     0.0000    0.0000    0.0000       266
       96.72     0.6638    0.4118    0.5083       187
       272.0     0.1765    0.0194    0.0349       155
       285.9     0.0000    0.0000    0.0000       247
       88.56     0.6494    0.6369    0.6431       157
       244.9     0.0000    0.0000    0.0000       210
         486     0.4839    0.0862    0.1463       174
       38.91     0.0000    0.0000    0.0000       148
       285.1     0.6154    0.0394    0.0741       203
       36.15     0.5867    0.9778    0.7333       135
       276.2     0.0000    0.0000    0.0000       195
         496     0.0000    0.0000    0.0000       160
       99.15     1.0000    0.0440    0.0842        91
      995.92     1.0000    0.0182    0.0357       165
      V58.61     0.0000    0.0000    0.0000       181
       507.0     0.8333    0.0490    0.0926       102
       038.9     0.5000    0.0134    0.0261       149
       88.72     0.0000    0.0000    0.0000        84
       585.9     0.0000    0.0000    0.0000       172
      403.90     0.0000    0.0000    0.0000       215
         311     0.0000    0.0000    0.0000       196
       305.1     0.0000    0.0000    0.0000       181
       37.22     0.5789    0.2200    0.3188       100
         412     0.0000    0.0000    0.0000       121
       33.24     0.0000    0.0000    0.0000       127
       39.95     0.8000    0.0556    0.1039        72
       287.5     0.0000    0.0000    0.0000       138
      410.71     0.4375    0.2308    0.3022        91
       276.1     0.0000    0.0000    0.0000       155
      V45.81     0.0000    0.0000    0.0000       108
       424.0     0.0000    0.0000    0.0000       104
       45.13     0.0000    0.0000    0.0000        87
      V15.82     0.0000    0.0000    0.0000       187
       511.9     0.0000    0.0000    0.0000        95
       37.23     0.3889    0.1167    0.1795        60

   micro avg     0.6304    0.2390    0.3466     10477
   macro avg     0.3417    0.1743    0.1951     10477
weighted avg     0.4077    0.2390    0.2682     10477
 samples avg     0.4557    0.2205    0.2767     10477

[0.6292654713707345, 0.43909774436090226, 0.33128976286871026]
目前最优验证集结果:0.34899

=== Epoch 0 end ===

=== Epoch 1 train ===
2023-08-16 22:57:41.526595

[MACRO] accuracy, precision, recall, f-measure
0.3773, 0.5386, 0.5092, 0.5235
[MICRO] accuracy, precision, recall, f-measure
0.4587, 0.6601, 0.6005, 0.6289
              precision    recall  f1-score   support

       401.9     0.6527    0.9237    0.7649       708
       38.93     0.5060    0.7538    0.6055       333
       428.0     0.7225    0.8576    0.7843       337
      427.31     0.8952    0.9495    0.9216       396
      414.01     0.8169    0.7706    0.7931       388
       96.04     0.6462    0.5526    0.5957       228
        96.6     0.6322    0.6800    0.6552       225
       584.9     0.5528    0.7705    0.6438       292
      250.00     0.6656    0.7509    0.7057       289
       96.71     0.6935    0.7887    0.7381       284
       272.4     0.6638    0.8105    0.7299       475
      518.81     0.6683    0.5472    0.6017       254
       99.04     0.0984    0.2222    0.1364        54
       39.61     0.9486    0.9760    0.9621       208
       599.0     0.7215    0.7383    0.7298       214
      530.81     0.7569    0.8074    0.7814       270
       96.72     0.5307    0.6786    0.5956       140
       272.0     0.7500    0.0199    0.0387       151
       285.9     0.2857    0.0190    0.0357       210
       88.56     0.7613    0.8806    0.8166       134
       244.9     0.7871    0.8833    0.8325       180
         486     0.5477    0.6855    0.6089       159
       38.91     0.5000    0.0310    0.0584       129
       285.1     0.6479    0.2434    0.3538       189
       36.15     0.8846    1.0000    0.9388       138
       276.2     0.3714    0.0774    0.1281       168
         496     0.5538    0.8308    0.6646       130
       99.15     0.4103    0.5818    0.4812        55
      995.92     0.6786    0.5672    0.6179       134
      V58.61     0.6013    0.5227    0.5593       176
       507.0     0.6947    0.6947    0.6947        95
       038.9     0.6296    0.4397    0.5178       116
       88.72     0.0000    0.0000    0.0000        96
       585.9     0.5556    0.4000    0.4651       150
      403.90     0.5412    0.5679    0.5542       162
         311     0.4545    0.0279    0.0526       179
       305.1     0.0000    0.0000    0.0000       167
       37.22     0.5087    0.9167    0.6543        96
         412     0.0000    0.0000    0.0000       112
       33.24     0.6897    0.2083    0.3200        96
       39.95     0.8696    0.9091    0.8889        88
       287.5     0.0000    0.0000    0.0000       138
      410.71     0.5000    0.3662    0.4228        71
       276.1     0.0000    0.0000    0.0000       129
      V45.81     0.8000    0.7500    0.7742        96
       424.0     0.6667    0.0278    0.0533        72
       45.13     0.5000    0.7927    0.6132        82
      V15.82     0.0000    0.0000    0.0000       157
       511.9     0.0000    0.0000    0.0000        85
       37.23     0.5676    0.4375    0.4941        48

   micro avg     0.6601    0.6005    0.6289      9283
   macro avg     0.5386    0.5092    0.4877      9283
weighted avg     0.5909    0.6005    0.5667      9283
 samples avg     0.6443    0.6005    0.5968      9283

[0.8270820089001907, 0.5988556897647807, 0.4217418944691672]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/llm_user/.conda/envs/torch39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/llm_user/.conda/envs/torch39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

Train step of epoch 1: 100%|██████████| 1345/1345 [1:08:11<00:00,  1.77s/it, loss=0.245]
Train step of epoch 1: 100%|██████████| 1345/1345 [1:08:11<00:00,  3.04s/it, loss=0.245]

[MACRO] accuracy, precision, recall, f-measure
0.3728, 0.5285, 0.5047, 0.5163
[MICRO] accuracy, precision, recall, f-measure
0.4561, 0.6579, 0.5978, 0.6264
              precision    recall  f1-score   support

       401.9     0.6484    0.9126    0.7581       778
       38.93     0.5255    0.7189    0.6071       402
       428.0     0.7974    0.8673    0.8309       422
      427.31     0.8898    0.9277    0.9083       470
      414.01     0.8256    0.7724    0.7981       435
       96.04     0.6442    0.5751    0.6077       233
        96.6     0.5581    0.7588    0.6431       228
       584.9     0.5921    0.7818    0.6738       362
      250.00     0.6328    0.7147    0.6713       340
       96.71     0.6115    0.7442    0.6713       258
       272.4     0.6792    0.7920    0.7313       548
      518.81     0.6622    0.5765    0.6164       255
       99.04     0.0877    0.1961    0.1212        51
       39.61     0.9648    0.9690    0.9669       226
       599.0     0.6691    0.7251    0.6960       251
      530.81     0.6891    0.8083    0.7439       266
       96.72     0.7016    0.7166    0.7090       187
       272.0     0.0000    0.0000    0.0000       155
       285.9     0.3333    0.0162    0.0309       247
       88.56     0.8343    0.8981    0.8650       157
       244.9     0.7815    0.8857    0.8304       210
         486     0.5567    0.6207    0.5870       174
       38.91     0.5000    0.0338    0.0633       148
       285.1     0.7561    0.3054    0.4351       203
       36.15     0.8710    1.0000    0.9310       135
       276.2     0.3871    0.0615    0.1062       195
         496     0.5252    0.7812    0.6281       160
       99.15     0.5182    0.6264    0.5672        91
      995.92     0.6835    0.5758    0.6250       165
      V58.61     0.6375    0.5635    0.5982       181
       507.0     0.6300    0.6176    0.6238       102
       038.9     0.5351    0.4094    0.4639       149
       88.72     0.5000    0.0119    0.0233        84
       585.9     0.4813    0.4477    0.4639       172
      403.90     0.5455    0.6698    0.6013       215
         311     0.3000    0.0153    0.0291       196
       305.1     0.0000    0.0000    0.0000       181
       37.22     0.4731    0.8800    0.6154       100
         412     0.0000    0.0000    0.0000       121
       33.24     0.7143    0.2756    0.3977       127
       39.95     0.8000    0.8889    0.8421        72
       287.5     0.0000    0.0000    0.0000       138
      410.71     0.5238    0.2418    0.3308        91
       276.1     0.0000    0.0000    0.0000       155
      V45.81     0.7723    0.7222    0.7464       108
       424.0     0.7143    0.0481    0.0901       104
       45.13     0.4610    0.8161    0.5892        87
      V15.82     0.0000    0.0000    0.0000       187
       511.9     0.0000    0.0000    0.0000        95
       37.23     0.4103    0.2667    0.3232        60

   micro avg     0.6579    0.5978    0.6264     10477
   macro avg     0.5285    0.5047    0.4832     10477
weighted avg     0.5827    0.5978    0.5651     10477
 samples avg     0.6480    0.6018    0.5975     10477

[0.8224407171775593, 0.6011567379988433, 0.42579525737420476]
目前最优验证集结果:0.62887

=== Epoch 1 end ===

=== Epoch 2 train ===
2023-08-17 00:05:52.610786

[MACRO] accuracy, precision, recall, f-measure
0.4848, 0.6427, 0.6390, 0.6409
[MICRO] accuracy, precision, recall, f-measure
0.5271, 0.6882, 0.6926, 0.6904
              precision    recall  f1-score   support

       401.9     0.8043    0.8997    0.8493       708
       38.93     0.5966    0.6396    0.6174       333
       428.0     0.7983    0.8457    0.8213       337
      427.31     0.9148    0.9495    0.9318       396
      414.01     0.8516    0.8428    0.8472       388
       96.04     0.5309    0.8289    0.6473       228
        96.6     0.8284    0.6222    0.7107       225
       584.9     0.7238    0.5925    0.6516       292
      250.00     0.6142    0.9308    0.7400       289
       96.71     0.7547    0.7042    0.7286       284
       272.4     0.7504    0.8863    0.8127       475
      518.81     0.6356    0.6181    0.6267       254
       99.04     0.0923    0.5556    0.1583        54
       39.61     0.8846    0.9952    0.9367       208
       599.0     0.8098    0.6963    0.7487       214
      530.81     0.7721    0.8407    0.8050       270
       96.72     0.5361    0.7429    0.6228       140
       272.0     0.7540    0.6291    0.6859       151
       285.9     0.0000    0.0000    0.0000       210
       88.56     0.7151    0.9179    0.8039       134
       244.9     0.7854    0.9556    0.8622       180
         486     0.5938    0.7170    0.6496       159
       38.91     0.4177    0.2558    0.3173       129
       285.1     0.7245    0.3757    0.4948       189
       36.15     0.9079    1.0000    0.9517       138
       276.2     0.5893    0.1964    0.2946       168
         496     0.6591    0.6692    0.6641       130
       99.15     0.8000    0.5091    0.6222        55
      995.92     0.7619    0.4776    0.5872       134
      V58.61     0.5504    0.8693    0.6740       176
       507.0     0.6702    0.6632    0.6667        95
       038.9     0.5149    0.5948    0.5520       116
       88.72     0.7632    0.3021    0.4328        96
       585.9     0.6644    0.6600    0.6622       150
      403.90     0.7518    0.6543    0.6997       162
         311     0.6242    0.5196    0.5671       179
       305.1     0.6301    0.6527    0.6412       167
       37.22     0.7143    0.4688    0.5660        96
         412     0.6212    0.7321    0.6721       112
       33.24     0.6552    0.5938    0.6230        96
       39.95     0.9121    0.9432    0.9274        88
       287.5     0.5747    0.3623    0.4444       138
      410.71     0.5094    0.7606    0.6102        71
       276.1     0.6022    0.4341    0.5045       129
      V45.81     0.8269    0.8958    0.8600        96
       424.0     0.5000    0.6250    0.5556        72
       45.13     0.6875    0.5366    0.6027        82
      V15.82     0.0000    0.0000    0.0000       157
       511.9     0.3784    0.1647    0.2295        85
       37.23     0.3750    0.6250    0.4688        48

   micro avg     0.6882    0.6926    0.6904      9283
   macro avg     0.6427    0.6390    0.6230      9283
weighted avg     0.6813    0.6926    0.6745      9283
 samples avg     0.6838    0.7072    0.6724      9283

[0.8588684043229498, 0.6432294977749523, 0.45759694850603944]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/llm_user/.conda/envs/torch39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/llm_user/.conda/envs/torch39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

Train step of epoch 2: 100%|██████████| 1345/1345 [1:08:13<00:00,  1.94s/it, loss=0.199]
Train step of epoch 2: 100%|██████████| 1345/1345 [1:08:13<00:00,  3.04s/it, loss=0.199]

[MACRO] accuracy, precision, recall, f-measure
0.4865, 0.6430, 0.6448, 0.6439
[MICRO] accuracy, precision, recall, f-measure
0.5260, 0.6839, 0.6950, 0.6894
              precision    recall  f1-score   support

       401.9     0.7814    0.9049    0.8386       778
       38.93     0.5938    0.6219    0.6075       402
       428.0     0.8404    0.8483    0.8443       422
      427.31     0.9029    0.9298    0.9161       470
      414.01     0.8433    0.8414    0.8423       435
       96.04     0.5508    0.8841    0.6787       233
        96.6     0.7285    0.7061    0.7171       228
       584.9     0.7542    0.6271    0.6848       362
      250.00     0.5927    0.9118    0.7184       340
       96.71     0.6642    0.6822    0.6730       258
       272.4     0.7528    0.8723    0.8081       548
      518.81     0.6307    0.7098    0.6679       255
       99.04     0.0997    0.7059    0.1748        51
       39.61     0.9146    0.9956    0.9534       226
       599.0     0.7808    0.6813    0.7277       251
      530.81     0.7166    0.8459    0.7759       266
       96.72     0.6141    0.7914    0.6916       187
       272.0     0.6496    0.5742    0.6096       155
       285.9     0.0000    0.0000    0.0000       247
       88.56     0.7869    0.9172    0.8471       157
       244.9     0.7634    0.9524    0.8475       210
         486     0.6477    0.6552    0.6514       174
       38.91     0.4300    0.2905    0.3468       148
       285.1     0.7436    0.4286    0.5437       203
       36.15     0.9060    1.0000    0.9507       135
       276.2     0.7101    0.2513    0.3712       195
         496     0.7054    0.5687    0.6298       160
       99.15     0.8750    0.6154    0.7226        91
      995.92     0.8113    0.5212    0.6347       165
      V58.61     0.5395    0.9061    0.6763       181
       507.0     0.6569    0.6569    0.6569       102
       038.9     0.5091    0.5638    0.5350       149
       88.72     0.5676    0.2500    0.3471        84
       585.9     0.5573    0.6221    0.5879       172
      403.90     0.7946    0.6837    0.7350       215
         311     0.5455    0.4898    0.5161       196
       305.1     0.6218    0.6630    0.6417       181
       37.22     0.6923    0.4500    0.5455       100
         412     0.6149    0.7521    0.6766       121
       33.24     0.7113    0.5433    0.6161       127
       39.95     0.8608    0.9444    0.9007        72
       287.5     0.5391    0.4493    0.4901       138
      410.71     0.4841    0.6703    0.5622        91
       276.1     0.7600    0.3677    0.4957       155
      V45.81     0.7874    0.9259    0.8511       108
       424.0     0.6139    0.5962    0.6049       104
       45.13     0.6098    0.5747    0.5917        87
      V15.82     0.0000    0.0000    0.0000       187
       511.9     0.5102    0.2632    0.3472        95
       37.23     0.3855    0.5333    0.4476        60

   micro avg     0.6839    0.6950    0.6894     10477
   macro avg     0.6430    0.6448    0.6260     10477
weighted avg     0.6775    0.6950    0.6740     10477
 samples avg     0.6825    0.7098    0.6722     10477

[0.843262001156738, 0.649277038750723, 0.46408328513591673]
目前最优验证集结果:0.69036

=== Epoch 2 end ===

=== Epoch 3 train ===
2023-08-17 01:14:05.825465

[MACRO] accuracy, precision, recall, f-measure
0.5039, 0.6546, 0.6694, 0.6619
[MICRO] accuracy, precision, recall, f-measure
0.5483, 0.6997, 0.7170, 0.7083
              precision    recall  f1-score   support

       401.9     0.8229    0.9124    0.8654       708
       38.93     0.5865    0.7027    0.6393       333
       428.0     0.8211    0.8309    0.8260       337
      427.31     0.9214    0.9470    0.9340       396
      414.01     0.8494    0.8866    0.8676       388
       96.04     0.5953    0.7807    0.6755       228
        96.6     0.8210    0.5911    0.6873       225
       584.9     0.7105    0.6473    0.6774       292
      250.00     0.7190    0.9031    0.8006       289
       96.71     0.6877    0.8063    0.7423       284
       272.4     0.7178    0.9158    0.8048       475
      518.81     0.6312    0.6535    0.6422       254
       99.04     0.1214    0.4630    0.1923        54
       39.61     0.8846    0.9952    0.9367       208
       599.0     0.5805    0.8084    0.6758       214
      530.81     0.7625    0.8444    0.8014       270
       96.72     0.7087    0.5214    0.6008       140
       272.0     0.7267    0.7219    0.7243       151
       285.9     0.3750    0.0286    0.0531       210
       88.56     0.7640    0.9179    0.8339       134
       244.9     0.8534    0.9056    0.8787       180
         486     0.5619    0.7421    0.6396       159
       38.91     0.3061    0.1163    0.1685       129
       285.1     0.7121    0.4974    0.5857       189
       36.15     0.8961    1.0000    0.9452       138
       276.2     0.6047    0.3095    0.4094       168
         496     0.5976    0.7538    0.6667       130
       99.15     0.4875    0.7091    0.5778        55
      995.92     0.6000    0.7164    0.6531       134
      V58.61     0.7018    0.6818    0.6916       176
       507.0     0.6034    0.7368    0.6635        95
       038.9     0.7200    0.3103    0.4337       116
       88.72     0.7455    0.4271    0.5430        96
       585.9     0.6853    0.6533    0.6689       150
      403.90     0.7744    0.6358    0.6983       162
         311     0.6165    0.7095    0.6597       179
       305.1     0.7662    0.3533    0.4836       167
       37.22     0.5987    0.9479    0.7339        96
         412     0.6746    0.7589    0.7143       112
       33.24     0.6509    0.7188    0.6832        96
       39.95     0.8646    0.9432    0.9022        88
       287.5     0.5405    0.4348    0.4819       138
      410.71     0.5657    0.7887    0.6588        71
       276.1     0.6531    0.4961    0.5639       129
      V45.81     0.8131    0.9062    0.8571        96
       424.0     0.5814    0.6944    0.6329        72
       45.13     0.6207    0.8780    0.7273        82
      V15.82     0.0000    0.0000    0.0000       157
       511.9     0.3769    0.5765    0.4558        85
       37.23     0.7500    0.1875    0.3000        48

   micro avg     0.6997    0.7170    0.7083      9283
   macro avg     0.6546    0.6694    0.6412      9283
weighted avg     0.6928    0.7170    0.6901      9283
 samples avg     0.7004    0.7307    0.6908      9283

[0.8684043229497775, 0.6586141131595677, 0.4660521296884933]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/llm_user/.conda/envs/torch39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/llm_user/.conda/envs/torch39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

Train step of epoch 3: 100%|██████████| 1345/1345 [1:08:32<00:00,  1.86s/it, loss=0.182]
Train step of epoch 3: 100%|██████████| 1345/1345 [1:08:32<00:00,  3.06s/it, loss=0.182]

[MACRO] accuracy, precision, recall, f-measure
0.4976, 0.6457, 0.6690, 0.6571
[MICRO] accuracy, precision, recall, f-measure
0.5431, 0.6926, 0.7157, 0.7039
              precision    recall  f1-score   support

       401.9     0.8039    0.9062    0.8520       778
       38.93     0.5711    0.6692    0.6163       402
       428.0     0.8632    0.8223    0.8422       422
      427.31     0.9064    0.9277    0.9169       470
      414.01     0.8384    0.8828    0.8600       435
       96.04     0.6006    0.8197    0.6933       233
        96.6     0.7488    0.6798    0.7126       228
       584.9     0.7349    0.6740    0.7032       362
      250.00     0.6973    0.8471    0.7649       340
       96.71     0.6257    0.8101    0.7061       258
       272.4     0.7261    0.9142    0.8094       548
      518.81     0.6573    0.7373    0.6950       255
       99.04     0.1272    0.5686    0.2079        51
       39.61     0.9036    0.9956    0.9474       226
       599.0     0.5788    0.8486    0.6882       251
      530.81     0.7220    0.8496    0.7807       266
       96.72     0.8146    0.6578    0.7278       187
       272.0     0.6341    0.6710    0.6520       155
       285.9     0.5000    0.0486    0.0886       247
       88.56     0.8314    0.9108    0.8693       157
       244.9     0.8268    0.9095    0.8662       210
         486     0.6276    0.7069    0.6649       174
       38.91     0.3810    0.1622    0.2275       148
       285.1     0.7266    0.4975    0.5906       203
       36.15     0.8766    1.0000    0.9343       135
       276.2     0.7041    0.3538    0.4710       195
         496     0.6201    0.6937    0.6549       160
       99.15     0.5238    0.7253    0.6083        91
      995.92     0.6531    0.7758    0.7091       165
      V58.61     0.6748    0.7680    0.7183       181
       507.0     0.5600    0.6863    0.6167       102
       038.9     0.6825    0.2886    0.4057       149
       88.72     0.5714    0.3810    0.4571        84
       585.9     0.5977    0.6047    0.6012       172
      403.90     0.8274    0.6465    0.7258       215
         311     0.5975    0.7194    0.6528       196
       305.1     0.7209    0.3425    0.4644       181
       37.22     0.5312    0.8500    0.6538       100
         412     0.6241    0.7273    0.6718       121
       33.24     0.7304    0.6614    0.6942       127
       39.95     0.8235    0.9722    0.8917        72
       287.5     0.4960    0.4493    0.4715       138
      410.71     0.5242    0.7143    0.6047        91
       276.1     0.7368    0.4516    0.5600       155
      V45.81     0.7407    0.9259    0.8230       108
       424.0     0.6842    0.6250    0.6533       104
       45.13     0.5203    0.8851    0.6553        87
      V15.82     0.0000    0.0000    0.0000       187
       511.9     0.3608    0.6000    0.4506        95
       37.23     0.4545    0.0833    0.1408        60

   micro avg     0.6926    0.7157    0.7039     10477
   macro avg     0.6457    0.6690    0.6355     10477
weighted avg     0.6898    0.7157    0.6866     10477
 samples avg     0.6950    0.7244    0.6856     10477

[0.8513591671486408, 0.6533256217466744, 0.47171775592828225]
目前最优验证集结果:0.70827

=== Epoch 3 end ===

=== Epoch 4 train ===
2023-08-17 02:22:37.918627

[MACRO] accuracy, precision, recall, f-measure
0.5208, 0.6669, 0.6679, 0.6674
[MICRO] accuracy, precision, recall, f-measure
0.5656, 0.7248, 0.7203, 0.7226
              precision    recall  f1-score   support

       401.9     0.8265    0.9153    0.8686       708
       38.93     0.5687    0.7087    0.6310       333
       428.0     0.7393    0.8754    0.8016       337
      427.31     0.9235    0.9444    0.9338       396
      414.01     0.8804    0.8918    0.8860       388
       96.04     0.6576    0.7412    0.6969       228
        96.6     0.7970    0.6978    0.7441       225
       584.9     0.7249    0.6678    0.6952       292
      250.00     0.7792    0.8547    0.8152       289
       96.71     0.7598    0.6127    0.6784       284
       272.4     0.7322    0.9095    0.8113       475
      518.81     0.6875    0.5197    0.5919       254
       99.04     0.2069    0.2222    0.2143        54
       39.61     0.9367    0.9952    0.9650       208
       599.0     0.8168    0.7290    0.7704       214
      530.81     0.7921    0.8185    0.8051       270
       96.72     0.6486    0.6857    0.6667       140
       272.0     0.7464    0.6821    0.7128       151
       285.9     0.4437    0.3190    0.3712       210
       88.56     0.7778    0.8881    0.8293       134
       244.9     0.8113    0.9556    0.8776       180
         486     0.5743    0.7296    0.6427       159
       38.91     0.0000    0.0000    0.0000       129
       285.1     0.6711    0.5291    0.5917       189
       36.15     0.9139    1.0000    0.9550       138
       276.2     0.5577    0.3452    0.4265       168
         496     0.6096    0.8769    0.7192       130
       99.15     0.7111    0.5818    0.6400        55
      995.92     0.6200    0.6940    0.6549       134
      V58.61     0.6779    0.8011    0.7344       176
       507.0     0.7407    0.6316    0.6818        95
       038.9     0.5952    0.4310    0.5000       116
       88.72     0.7500    0.4062    0.5270        96
       585.9     0.6243    0.7200    0.6687       150
      403.90     0.7600    0.7037    0.7308       162
         311     0.5992    0.8603    0.7064       179
       305.1     0.6010    0.6946    0.6444       167
       37.22     0.7258    0.4688    0.5696        96
         412     0.6357    0.7946    0.7063       112
       33.24     0.7703    0.5938    0.6706        96
       39.95     0.8557    0.9432    0.8973        88
       287.5     0.6615    0.3116    0.4236       138
      410.71     0.7143    0.7042    0.7092        71
       276.1     0.6562    0.4884    0.5600       129
      V45.81     0.8614    0.9062    0.8832        96
       424.0     0.7818    0.5972    0.6772        72
       45.13     0.6195    0.8537    0.7179        82
      V15.82     0.0000    0.0000    0.0000       157
       511.9     0.4493    0.3647    0.4026        85
       37.23     0.3500    0.7292    0.4730        48

   micro avg     0.7248    0.7203    0.7226      9283
   macro avg     0.6669    0.6679    0.6576      9283
weighted avg     0.7038    0.7203    0.7047      9283
 samples avg     0.7227    0.7360    0.7050      9283

[0.8785759694850604, 0.6625556261919898, 0.46656071201525745]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/llm_user/.conda/envs/torch39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/llm_user/.conda/envs/torch39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

Train step of epoch 4: 100%|██████████| 1345/1345 [1:08:30<00:00,  1.94s/it, loss=0.171]
Train step of epoch 4: 100%|██████████| 1345/1345 [1:08:30<00:00,  3.06s/it, loss=0.171]

[MACRO] accuracy, precision, recall, f-measure
0.5162, 0.6633, 0.6668, 0.6650
[MICRO] accuracy, precision, recall, f-measure
0.5616, 0.7213, 0.7173, 0.7193
              precision    recall  f1-score   support

       401.9     0.8150    0.9062    0.8582       778
       38.93     0.5828    0.7090    0.6397       402
       428.0     0.8154    0.8791    0.8461       422
      427.31     0.9083    0.9277    0.9179       470
      414.01     0.8604    0.8644    0.8624       435
       96.04     0.6397    0.7468    0.6891       233
        96.6     0.7097    0.7719    0.7395       228
       584.9     0.7429    0.6464    0.6913       362
      250.00     0.7644    0.7824    0.7733       340
       96.71     0.7455    0.6357    0.6862       258
       272.4     0.7410    0.9033    0.8141       548
      518.81     0.7536    0.6118    0.6753       255
       99.04     0.2000    0.2745    0.2314        51
       39.61     0.9494    0.9956    0.9719       226
       599.0     0.7673    0.7490    0.7581       251
      530.81     0.7586    0.8271    0.7914       266
       96.72     0.7667    0.7380    0.7520       187
       272.0     0.6433    0.6516    0.6474       155
       285.9     0.4479    0.2955    0.3561       247
       88.56     0.8452    0.9045    0.8738       157
       244.9     0.8016    0.9429    0.8665       210
         486     0.6218    0.6897    0.6540       174
       38.91     0.0000    0.0000    0.0000       148
       285.1     0.6558    0.4975    0.5658       203
       36.15     0.9060    1.0000    0.9507       135
       276.2     0.5965    0.3487    0.4401       195
         496     0.5909    0.8125    0.6842       160
       99.15     0.7250    0.6374    0.6784        91
      995.92     0.6269    0.7333    0.6760       165
      V58.61     0.6218    0.8177    0.7064       181
       507.0     0.7059    0.5882    0.6417       102
       038.9     0.6117    0.4228    0.5000       149
       88.72     0.6327    0.3690    0.4662        84
       585.9     0.5439    0.7209    0.6200       172
      403.90     0.8061    0.7349    0.7689       215
         311     0.5631    0.8418    0.6748       196
       305.1     0.6281    0.6906    0.6579       181
       37.22     0.6875    0.4400    0.5366       100
         412     0.6289    0.8264    0.7143       121
       33.24     0.8353    0.5591    0.6698       127
       39.95     0.8235    0.9722    0.8917        72
       287.5     0.5930    0.3696    0.4554       138
      410.71     0.6944    0.5495    0.6135        91
       276.1     0.7444    0.4323    0.5469       155
      V45.81     0.7937    0.9259    0.8547       108
       424.0     0.7714    0.5192    0.6207       104
       45.13     0.5507    0.8736    0.6756        87
      V15.82     0.0000    0.0000    0.0000       187
       511.9     0.5309    0.4526    0.4886        95
       37.23     0.4167    0.7500    0.5357        60

   micro avg     0.7213    0.7173    0.7193     10477
   macro avg     0.6633    0.6668    0.6546     10477
weighted avg     0.7007    0.7173    0.7015     10477
 samples avg     0.7208    0.7320    0.7026     10477

[0.8768074031231926, 0.6661654135338346, 0.47651821862348176]
目前最优验证集结果:0.72257

=== Epoch 4 end ===

=== Epoch 5 train ===
2023-08-17 03:31:07.926551

[MACRO] accuracy, precision, recall, f-measure
0.5205, 0.6819, 0.6665, 0.6741
[MICRO] accuracy, precision, recall, f-measure
0.5679, 0.7248, 0.7240, 0.7244
              precision    recall  f1-score   support

       401.9     0.8153    0.9167    0.8630       708
       38.93     0.6027    0.6607    0.6304       333
       428.0     0.8658    0.8042    0.8338       337
      427.31     0.9185    0.9394    0.9288       396
      414.01     0.8689    0.8711    0.8700       388
       96.04     0.6595    0.8070    0.7258       228
        96.6     0.7673    0.6889    0.7260       225
       584.9     0.6614    0.7158    0.6875       292
      250.00     0.7550    0.9170    0.8281       289
       96.71     0.7447    0.7394    0.7420       284
       272.4     0.7404    0.8947    0.8103       475
      518.81     0.5823    0.7520    0.6564       254
       99.04     0.1474    0.2593    0.1879        54
       39.61     0.9442    0.9760    0.9598       208
       599.0     0.7816    0.7523    0.7667       214
      530.81     0.7643    0.8407    0.8007       270
       96.72     0.6623    0.7143    0.6873       140
       272.0     0.7115    0.7351    0.7231       151
       285.9     0.5385    0.1667    0.2545       210
       88.56     0.7867    0.8806    0.8310       134
       244.9     0.8047    0.9611    0.8759       180
         486     0.6618    0.5660    0.6102       159
       38.91     0.5714    0.1240    0.2038       129
       285.1     0.7526    0.3862    0.5105       189
       36.15     0.9200    1.0000    0.9583       138
       276.2     0.5833    0.3333    0.4242       168
         496     0.6687    0.8538    0.7500       130
       99.15     0.7353    0.4545    0.5618        55
      995.92     0.6724    0.5821    0.6240       134
      V58.61     0.6745    0.8125    0.7371       176
       507.0     0.7439    0.6421    0.6893        95
       038.9     0.6092    0.4569    0.5222       116
       88.72     0.8333    0.2604    0.3968        96
       585.9     0.6140    0.7000    0.6542       150
      403.90     0.7039    0.7778    0.7390       162
         311     0.6000    0.8547    0.7051       179
       305.1     0.5579    0.7784    0.6500       167
       37.22     0.6863    0.7292    0.7071        96
         412     0.6762    0.6339    0.6544       112
       33.24     0.6486    0.7500    0.6957        96
       39.95     0.9111    0.9318    0.9213        88
       287.5     0.7708    0.2681    0.3978       138
      410.71     0.6883    0.7465    0.7162        71
       276.1     0.6458    0.4806    0.5511       129
      V45.81     0.8600    0.8958    0.8776        96
       424.0     0.6098    0.6944    0.6494        72
       45.13     0.6283    0.8659    0.7282        82
      V15.82     0.0000    0.0000    0.0000       157
       511.9     0.3824    0.4588    0.4171        85
       37.23     0.5600    0.2917    0.3836        48

   micro avg     0.7248    0.7240    0.7244      9283
   macro avg     0.6819    0.6665    0.6565      9283
weighted avg     0.7140    0.7240    0.7055      9283
 samples avg     0.7196    0.7324    0.7024      9283

[0.8798474253019708, 0.6678957406230134, 0.46929434202161474]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/llm_user/.conda/envs/torch39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/llm_user/.conda/envs/torch39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

Train step of epoch 5: 100%|██████████| 1345/1345 [1:08:26<00:00,  1.87s/it, loss=0.164]
Train step of epoch 5: 100%|██████████| 1345/1345 [1:08:26<00:00,  3.05s/it, loss=0.164]

[MACRO] accuracy, precision, recall, f-measure
0.5177, 0.6777, 0.6664, 0.6720
[MICRO] accuracy, precision, recall, f-measure
0.5630, 0.7192, 0.7216, 0.7204
              precision    recall  f1-score   support

       401.9     0.8078    0.9023    0.8525       778
       38.93     0.6150    0.6517    0.6329       402
       428.0     0.9057    0.7962    0.8474       422
      427.31     0.8994    0.9319    0.9154       470
      414.01     0.8703    0.8483    0.8591       435
       96.04     0.6447    0.8412    0.7300       233
        96.6     0.6848    0.7719    0.7258       228
       584.9     0.6954    0.7127    0.7040       362
      250.00     0.7225    0.8882    0.7968       340
       96.71     0.6751    0.7248    0.6991       258
       272.4     0.7473    0.8905    0.8127       548
      518.81     0.5869    0.8078    0.6799       255
       99.04     0.1346    0.2745    0.1806        51
       39.61     0.9733    0.9690    0.9712       226
       599.0     0.7280    0.7570    0.7422       251
      530.81     0.6879    0.8534    0.7617       266
       96.72     0.7330    0.7487    0.7407       187
       272.0     0.6185    0.6903    0.6524       155
       285.9     0.4932    0.1457    0.2250       247
       88.56     0.8304    0.9045    0.8659       157
       244.9     0.7928    0.9476    0.8633       210
         486     0.7652    0.5805    0.6601       174
       38.91     0.5385    0.0946    0.1609       148
       285.1     0.8191    0.3793    0.5185       203
       36.15     0.9247    1.0000    0.9609       135
       276.2     0.7474    0.3641    0.4897       195
         496     0.6684    0.8063    0.7309       160
       99.15     0.8485    0.6154    0.7134        91
      995.92     0.7368    0.6788    0.7066       165
      V58.61     0.6078    0.8564    0.7110       181
       507.0     0.7262    0.5980    0.6559       102
       038.9     0.6224    0.4094    0.4939       149
       88.72     0.6000    0.2143    0.3158        84
       585.9     0.5874    0.7035    0.6402       172
      403.90     0.7706    0.7814    0.7760       215
         311     0.5795    0.8367    0.6848       196
       305.1     0.5343    0.8177    0.6463       181
       37.22     0.6733    0.6800    0.6766       100
         412     0.6422    0.5785    0.6087       121
       33.24     0.6769    0.6929    0.6848       127
       39.95     0.8846    0.9583    0.9200        72
       287.5     0.6286    0.3188    0.4231       138
      410.71     0.6867    0.6264    0.6552        91
       276.1     0.6900    0.4452    0.5412       155
      V45.81     0.8065    0.9259    0.8621       108
       424.0     0.7188    0.6635    0.6900       104
       45.13     0.6080    0.8736    0.7170        87
      V15.82     0.0000    0.0000    0.0000       187
       511.9     0.4031    0.5474    0.4643        95
       37.23     0.5417    0.2167    0.3095        60

   micro avg     0.7192    0.7216    0.7204     10477
   macro avg     0.6777    0.6664    0.6535     10477
weighted avg     0.7098    0.7216    0.7013     10477
 samples avg     0.7121    0.7309    0.6985     10477

[0.8744939271255061, 0.6690572585309428, 0.4751301330248699]
目前最优验证集结果:0.72440

=== Epoch 5 end ===

=== Epoch 6 train ===
2023-08-17 04:39:34.129563

[MACRO] accuracy, precision, recall, f-measure
0.5323, 0.6864, 0.6787, 0.6825
[MICRO] accuracy, precision, recall, f-measure
0.5719, 0.7324, 0.7230, 0.7277
              precision    recall  f1-score   support

       401.9     0.8213    0.9025    0.8600       708
       38.93     0.5960    0.6336    0.6143       333
       428.0     0.8160    0.8160    0.8160       337
      427.31     0.9277    0.9394    0.9335       396
      414.01     0.8863    0.8840    0.8852       388
       96.04     0.7118    0.7149    0.7133       228
        96.6     0.7892    0.6489    0.7122       225
       584.9     0.7446    0.7089    0.7263       292
      250.00     0.7950    0.8720    0.8317       289
       96.71     0.7543    0.6162    0.6783       284
       272.4     0.7531    0.8863    0.8143       475
      518.81     0.7166    0.5276    0.6077       254
       99.04     0.1562    0.0926    0.1163        54
       39.61     0.9367    0.9952    0.9650       208
       599.0     0.7746    0.7710    0.7728       214
      530.81     0.7661    0.8370    0.8000       270
       96.72     0.6456    0.7286    0.6846       140
       272.0     0.7481    0.6689    0.7063       151
       285.9     0.4951    0.2429    0.3259       210
       88.56     0.7987    0.8881    0.8410       134
       244.9     0.8269    0.9556    0.8866       180
         486     0.6283    0.7547    0.6857       159
       38.91     0.6190    0.2016    0.3041       129
       285.1     0.6625    0.5608    0.6074       189
       36.15     0.9514    0.9928    0.9716       138
       276.2     0.5844    0.2679    0.3673       168
         496     0.6943    0.8385    0.7596       130
       99.15     0.6275    0.5818    0.6038        55
      995.92     0.5316    0.7537    0.6235       134
      V58.61     0.6862    0.7330    0.7088       176
       507.0     0.6882    0.6737    0.6809        95
       038.9     0.4695    0.6638    0.5500       116
       88.72     0.6296    0.5312    0.5763        96
       585.9     0.7031    0.6000    0.6475       150
      403.90     0.7222    0.7222    0.7222       162
         311     0.6213    0.8156    0.7053       179
       305.1     0.6011    0.6766    0.6366       167
       37.22     0.7207    0.8333    0.7729        96
         412     0.6377    0.7857    0.7040       112
       33.24     0.7000    0.6562    0.6774        96
       39.95     0.8913    0.9318    0.9111        88
       287.5     0.5111    0.5000    0.5055       138
      410.71     0.7463    0.7042    0.7246        71
       276.1     0.5856    0.5039    0.5417       129
      V45.81     0.8776    0.8958    0.8866        96
       424.0     0.5843    0.7222    0.6460        72
       45.13     0.5814    0.9146    0.7109        82
      V15.82     0.4286    0.0191    0.0366       157
       511.9     0.5085    0.3529    0.4167        85
       37.23     0.6667    0.4167    0.5128        48

   micro avg     0.7324    0.7230    0.7277      9283
   macro avg     0.6864    0.6787    0.6698      9283
weighted avg     0.7232    0.7230    0.7128      9283
 samples avg     0.7300    0.7376    0.7098      9283

[0.8684043229497775, 0.6678957406230134, 0.4708836617927527]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/llm_user/.conda/envs/torch39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

Train step of epoch 6: 100%|██████████| 1345/1345 [1:08:40<00:00,  1.80s/it, loss=0.155]
Train step of epoch 6: 100%|██████████| 1345/1345 [1:08:40<00:00,  3.06s/it, loss=0.155]

[MACRO] accuracy, precision, recall, f-measure
0.5230, 0.6820, 0.6718, 0.6768
[MICRO] accuracy, precision, recall, f-measure
0.5653, 0.7262, 0.7183, 0.7223
              precision    recall  f1-score   support

       401.9     0.8144    0.8856    0.8485       778
       38.93     0.6128    0.6418    0.6270       402
       428.0     0.8741    0.8223    0.8474       422
      427.31     0.9066    0.9298    0.9181       470
      414.01     0.8552    0.8690    0.8620       435
       96.04     0.6996    0.7597    0.7284       233
        96.6     0.7017    0.7325    0.7167       228
       584.9     0.7530    0.6906    0.7205       362
      250.00     0.7771    0.8000    0.7884       340
       96.71     0.7431    0.6279    0.6807       258
       272.4     0.7570    0.8869    0.8168       548
      518.81     0.7725    0.5725    0.6577       255
       99.04     0.2333    0.1373    0.1728        51
       39.61     0.9534    0.9956    0.9740       226
       599.0     0.7358    0.7769    0.7558       251
      530.81     0.7040    0.8496    0.7700       266
       96.72     0.7174    0.7059    0.7116       187
       272.0     0.6233    0.5871    0.6047       155
       285.9     0.4297    0.2227    0.2933       247
       88.56     0.8662    0.8662    0.8662       157
       244.9     0.8000    0.9333    0.8615       210
         486     0.6497    0.6609    0.6553       174
       38.91     0.6034    0.2365    0.3398       148
       285.1     0.6708    0.5320    0.5934       203
       36.15     0.9437    0.9926    0.9675       135
       276.2     0.7234    0.3487    0.4706       195
         496     0.6566    0.8125    0.7263       160
       99.15     0.6818    0.6593    0.6704        91
      995.92     0.6291    0.8121    0.7090       165
      V58.61     0.6368    0.7845    0.7030       181
       507.0     0.6809    0.6275    0.6531       102
       038.9     0.5101    0.6779    0.5821       149
       88.72     0.4868    0.4405    0.4625        84
       585.9     0.6626    0.6279    0.6448       172
      403.90     0.7561    0.7209    0.7381       215
         311     0.5789    0.7857    0.6667       196
       305.1     0.6117    0.6961    0.6512       181
       37.22     0.6048    0.7500    0.6696       100
         412     0.6438    0.7769    0.7041       121
       33.24     0.7767    0.6299    0.6957       127
       39.95     0.8500    0.9444    0.8947        72
       287.5     0.4768    0.5217    0.4983       138
      410.71     0.6709    0.5824    0.6235        91
       276.1     0.6786    0.4903    0.5693       155
      V45.81     0.7920    0.9167    0.8498       108
       424.0     0.6514    0.6827    0.6667       104
       45.13     0.5197    0.9080    0.6611        87
      V15.82     0.4286    0.0160    0.0309       187
       511.9     0.5625    0.3789    0.4528        95
       37.23     0.6296    0.2833    0.3908        60

   micro avg     0.7262    0.7183    0.7223     10477
   macro avg     0.6820    0.6718    0.6633     10477
weighted avg     0.7188    0.7183    0.7080     10477
 samples avg     0.7226    0.7322    0.7045     10477

[0.876229034123771, 0.6666281087333719, 0.47802197802197804]
目前最优验证集结果:0.72767

=== Epoch 6 end ===

=== Epoch 7 train ===
2023-08-17 05:48:14.230056

[MACRO] accuracy, precision, recall, f-measure
0.5235, 0.6790, 0.6705, 0.6747
[MICRO] accuracy, precision, recall, f-measure
0.5624, 0.7254, 0.7144, 0.7199
              precision    recall  f1-score   support

       401.9     0.8188    0.8997    0.8573       708
       38.93     0.6141    0.5736    0.5932       333
       428.0     0.7911    0.8427    0.8161       337
      427.31     0.9322    0.9369    0.9345       396
      414.01     0.8612    0.8634    0.8623       388
       96.04     0.6237    0.8070    0.7036       228
        96.6     0.7012    0.7822    0.7395       225
       584.9     0.6500    0.7568    0.6994       292
      250.00     0.7622    0.8651    0.8104       289
       96.71     0.7592    0.6549    0.7032       284
       272.4     0.7627    0.8526    0.8052       475
      518.81     0.6562    0.5787    0.6151       254
       99.04     0.2278    0.3333    0.2707        54
       39.61     0.9283    0.9952    0.9606       208
       599.0     0.7639    0.7710    0.7674       214
      530.81     0.7885    0.8148    0.8015       270
       96.72     0.5450    0.7357    0.6261       140
       272.0     0.7807    0.5894    0.6717       151
       285.9     0.5000    0.1333    0.2105       210
       88.56     0.7442    0.9552    0.8366       134
       244.9     0.8284    0.9389    0.8802       180
         486     0.6022    0.6855    0.6412       159
       38.91     0.7000    0.1085    0.1879       129
       285.1     0.6579    0.5291    0.5865       189
       36.15     0.9448    0.9928    0.9682       138
       276.2     0.4472    0.4286    0.4377       168
         496     0.7676    0.8385    0.8015       130
       99.15     0.6667    0.5455    0.6000        55
      995.92     0.6014    0.6418    0.6209       134
      V58.61     0.7181    0.7670    0.7418       176
       507.0     0.6667    0.6737    0.6702        95
       038.9     0.5273    0.5000    0.5133       116
       88.72     0.8261    0.3958    0.5352        96
       585.9     0.7319    0.6733    0.7014       150
      403.90     0.7877    0.7099    0.7468       162
         311     0.6771    0.7263    0.7008       179
       305.1     0.6697    0.4371    0.5290       167
       37.22     0.7238    0.7917    0.7562        96
         412     0.6111    0.6875    0.6471       112
       33.24     0.6952    0.7604    0.7264        96
       39.95     0.8384    0.9432    0.8877        88
       287.5     0.6067    0.3913    0.4758       138
      410.71     0.6500    0.7324    0.6887        71
       276.1     0.6484    0.4574    0.5364       129
      V45.81     0.8431    0.8958    0.8687        96
       424.0     0.6301    0.6389    0.6345        72
       45.13     0.6000    0.9146    0.7246        82
      V15.82     0.0000    0.0000    0.0000       157
       511.9     0.3636    0.5176    0.4272        85
       37.23     0.7097    0.4583    0.5570        48

   micro avg     0.7254    0.7144    0.7199      9283
   macro avg     0.6790    0.6705    0.6615      9283
weighted avg     0.7115    0.7144    0.7029      9283
 samples avg     0.7178    0.7264    0.6993      9283

[0.8798474253019708, 0.6654799745708837, 0.4684043229497775]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/llm_user/.conda/envs/torch39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

Train step of epoch 7: 100%|██████████| 1345/1345 [1:08:01<00:00,  1.95s/it, loss=0.144]
Train step of epoch 7: 100%|██████████| 1345/1345 [1:08:01<00:00,  3.03s/it, loss=0.144]

[MACRO] accuracy, precision, recall, f-measure
0.5153, 0.6923, 0.6619, 0.6768
[MICRO] accuracy, precision, recall, f-measure
0.5563, 0.7212, 0.7087, 0.7149
              precision    recall  f1-score   support

       401.9     0.7998    0.8933    0.8440       778
       38.93     0.6160    0.5547    0.5838       402
       428.0     0.8435    0.8555    0.8494       422
      427.31     0.9119    0.9255    0.9187       470
      414.01     0.8633    0.8276    0.8451       435
       96.04     0.6424    0.8326    0.7252       233
        96.6     0.6332    0.8026    0.7079       228
       584.9     0.6707    0.7707    0.7172       362
      250.00     0.7467    0.8412    0.7911       340
       96.71     0.7155    0.6628    0.6881       258
       272.4     0.7643    0.8285    0.7951       548
      518.81     0.6992    0.6745    0.6866       255
       99.04     0.1882    0.3137    0.2353        51
       39.61     0.9492    0.9912    0.9697       226
       599.0     0.7046    0.7888    0.7444       251
      530.81     0.7333    0.8271    0.7774       266
       96.72     0.6787    0.8021    0.7353       187
       272.0     0.6535    0.5355    0.5887       155
       285.9     0.4535    0.1579    0.2342       247
       88.56     0.8000    0.9172    0.8546       157
       244.9     0.8268    0.9095    0.8662       210
         486     0.7059    0.6897    0.6977       174
       38.91     0.6429    0.1216    0.2045       148
       285.1     0.6506    0.5320    0.5854       203
       36.15     0.9504    0.9926    0.9710       135
       276.2     0.4583    0.3949    0.4242       195
         496     0.7574    0.8000    0.7781       160
       99.15     0.7317    0.6593    0.6936        91
      995.92     0.7000    0.7212    0.7104       165
      V58.61     0.6604    0.7735    0.7125       181
       507.0     0.5872    0.6275    0.6066       102
       038.9     0.5746    0.5168    0.5442       149
       88.72     0.7436    0.3452    0.4715        84
       585.9     0.6772    0.6221    0.6485       172
      403.90     0.8261    0.7070    0.7619       215
         311     0.6037    0.6684    0.6344       196
       305.1     0.6583    0.4365    0.5249       181
       37.22     0.7174    0.6600    0.6875       100
         412     0.6000    0.6942    0.6437       121
       33.24     0.7190    0.6850    0.7016       127
       39.95     0.8046    0.9722    0.8805        72
       287.5     0.5625    0.4565    0.5040       138
      410.71     0.6737    0.7033    0.6882        91
       276.1     0.7174    0.4258    0.5344       155
      V45.81     0.7674    0.9167    0.8354       108
       424.0     0.7500    0.6058    0.6702       104
       45.13     0.4968    0.8966    0.6393        87
      V15.82     1.0000    0.0053    0.0106       187
       511.9     0.3840    0.5053    0.4364        95
       37.23     0.6000    0.2500    0.3529        60

   micro avg     0.7212    0.7087    0.7149     10477
   macro avg     0.6923    0.6619    0.6542     10477
weighted avg     0.7230    0.7087    0.6981     10477
 samples avg     0.7177    0.7203    0.6962     10477

[0.8768074031231926, 0.6691729323308271, 0.4759976865240023]
目前最优验证集结果:0.72767

=== Epoch 7 end ===

=== Epoch 8 train ===
2023-08-17 06:56:15.534992

[MACRO] accuracy, precision, recall, f-measure
0.5268, 0.6986, 0.6601, 0.6788
[MICRO] accuracy, precision, recall, f-measure
0.5615, 0.7411, 0.6986, 0.7192
              precision    recall  f1-score   support

       401.9     0.8129    0.9082    0.8579       708
       38.93     0.6598    0.4775    0.5540       333
       428.0     0.8399    0.8249    0.8323       337
      427.31     0.9271    0.9318    0.9295       396
      414.01     0.8458    0.9046    0.8742       388
       96.04     0.7596    0.6930    0.7248       228
        96.6     0.7288    0.7644    0.7462       225
       584.9     0.7283    0.6884    0.7077       292
      250.00     0.7719    0.8547    0.8112       289
       96.71     0.7212    0.6831    0.7016       284
       272.4     0.7600    0.8400    0.7980       475
      518.81     0.7923    0.4055    0.5365       254
       99.04     0.2267    0.3148    0.2636        54
       39.61     0.9628    0.9952    0.9787       208
       599.0     0.7642    0.7570    0.7606       214
      530.81     0.7920    0.8037    0.7978       270
       96.72     0.7107    0.6143    0.6590       140
       272.0     0.7299    0.6623    0.6944       151
       285.9     0.5104    0.2333    0.3203       210
       88.56     0.8121    0.9030    0.8551       134
       244.9     0.8770    0.9111    0.8937       180
         486     0.6322    0.6918    0.6607       159
       38.91     0.5806    0.2791    0.3770       129
       285.1     0.6559    0.6455    0.6507       189
       36.15     0.9648    0.9928    0.9786       138
       276.2     0.6400    0.1905    0.2936       168
         496     0.7833    0.7231    0.7520       130
       99.15     0.7561    0.5636    0.6458        55
      995.92     0.6176    0.6269    0.6222       134
      V58.61     0.6667    0.7955    0.7254       176
       507.0     0.6569    0.7053    0.6802        95
       038.9     0.4921    0.5345    0.5124       116
       88.72     0.8800    0.2292    0.3636        96
       585.9     0.6429    0.7200    0.6792       150
      403.90     0.6927    0.7654    0.7273       162
         311     0.7290    0.4358    0.5455       179
       305.1     0.7238    0.4551    0.5588       167
       37.22     0.8021    0.8021    0.8021        96
         412     0.6138    0.7946    0.6926       112
       33.24     0.7763    0.6146    0.6860        96
       39.95     0.9011    0.9318    0.9162        88
       287.5     0.6322    0.3986    0.4889       138
      410.71     0.6163    0.7465    0.6752        71
       276.1     0.6667    0.4651    0.5479       129
      V45.81     0.8763    0.8854    0.8808        96
       424.0     0.5517    0.6667    0.6038        72
       45.13     0.6939    0.8293    0.7556        82
      V15.82     0.0000    0.0000    0.0000       157
       511.9     0.2654    0.6588    0.3784        85
       37.23     0.6875    0.6875    0.6875        48

   micro avg     0.7411    0.6986    0.7192      9283
   macro avg     0.6986    0.6601    0.6637      9283
weighted avg     0.7312    0.6986    0.7025      9283
 samples avg     0.7308    0.7072    0.6946      9283

[0.8728544183089637, 0.6650985378258105, 0.4645263827082009]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/llm_user/.conda/envs/torch39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

Train step of epoch 8: 100%|██████████| 1345/1345 [1:08:07<00:00,  1.90s/it, loss=0.133]
Train step of epoch 8: 100%|██████████| 1345/1345 [1:08:07<00:00,  3.04s/it, loss=0.133]

[MACRO] accuracy, precision, recall, f-measure
0.5157, 0.7062, 0.6528, 0.6784
[MICRO] accuracy, precision, recall, f-measure
0.5508, 0.7290, 0.6927, 0.7104
              precision    recall  f1-score   support

       401.9     0.8005    0.8869    0.8415       778
       38.93     0.6446    0.4602    0.5370       402
       428.0     0.8688    0.8318    0.8499       422
      427.31     0.9072    0.9149    0.9110       470
      414.01     0.8269    0.8897    0.8571       435
       96.04     0.7273    0.7210    0.7241       233
        96.6     0.6786    0.8333    0.7480       228
       584.9     0.7600    0.6823    0.7191       362
      250.00     0.7729    0.8206    0.7960       340
       96.71     0.6859    0.7364    0.7103       258
       272.4     0.7666    0.8212    0.7930       548
      518.81     0.8487    0.3961    0.5401       255
       99.04     0.2615    0.3333    0.2931        51
       39.61     0.9782    0.9912    0.9846       226
       599.0     0.7029    0.7729    0.7362       251
      530.81     0.7416    0.8308    0.7837       266
       96.72     0.7847    0.6043    0.6828       187
       272.0     0.6316    0.6194    0.6254       155
       285.9     0.4846    0.2551    0.3342       247
       88.56     0.8491    0.8599    0.8544       157
       244.9     0.8507    0.8952    0.8724       210
         486     0.6667    0.6322    0.6490       174
       38.91     0.5342    0.2635    0.3529       148
       285.1     0.6316    0.5911    0.6107       203
       36.15     0.9306    0.9926    0.9606       135
       276.2     0.7465    0.2718    0.3985       195
         496     0.8069    0.7312    0.7672       160
       99.15     0.7778    0.6154    0.6871        91
      995.92     0.7233    0.6970    0.7099       165
      V58.61     0.5873    0.8177    0.6836       181
       507.0     0.5776    0.6569    0.6147       102
       038.9     0.5714    0.5369    0.5536       149
       88.72     0.6429    0.2143    0.3214        84
       585.9     0.5602    0.7035    0.6237       172
      403.90     0.7555    0.8047    0.7793       215
         311     0.6371    0.4031    0.4938       196
       305.1     0.6609    0.4199    0.5135       181
       37.22     0.6731    0.7000    0.6863       100
         412     0.6145    0.8430    0.7108       121
       33.24     0.8298    0.6142    0.7059       127
       39.95     0.9079    0.9583    0.9324        72
       287.5     0.5495    0.4420    0.4900       138
      410.71     0.6598    0.7033    0.6809        91
       276.1     0.7222    0.4194    0.5306       155
      V45.81     0.7899    0.8704    0.8282       108
       424.0     0.6701    0.6250    0.6468       104
       45.13     0.6606    0.8276    0.7347        87
      V15.82     1.0000    0.0053    0.0106       187
       511.9     0.2481    0.6737    0.3626        95
       37.23     0.6000    0.4500    0.5143        60

   micro avg     0.7290    0.6927    0.7104     10477
   macro avg     0.7062    0.6528    0.6549     10477
weighted avg     0.7383    0.6927    0.6950     10477
 samples avg     0.7266    0.7032    0.6905     10477

[0.8692886061307113, 0.6643146327356854, 0.47270098322729903]
目前最优验证集结果:0.72767

=== Epoch 8 end ===

=== Epoch 9 train ===
2023-08-17 08:04:23.168979

[MACRO] accuracy, precision, recall, f-measure
0.5285, 0.6855, 0.6676, 0.6764
[MICRO] accuracy, precision, recall, f-measure
0.5657, 0.7338, 0.7117, 0.7226
              precision    recall  f1-score   support

       401.9     0.8170    0.8955    0.8544       708
       38.93     0.6594    0.4535    0.5374       333
       428.0     0.8132    0.8398    0.8263       337
      427.31     0.9271    0.9318    0.9295       396
      414.01     0.8928    0.8582    0.8752       388
       96.04     0.7323    0.6360    0.6808       228
        96.6     0.7404    0.6844    0.7113       225
       584.9     0.7025    0.7603    0.7303       292
      250.00     0.7656    0.8478    0.8046       289
       96.71     0.7645    0.6514    0.7034       284
       272.4     0.7409    0.8968    0.8114       475
      518.81     0.6458    0.6102    0.6275       254
       99.04     0.2500    0.1667    0.2000        54
       39.61     0.9539    0.9952    0.9741       208
       599.0     0.7919    0.7290    0.7591       214
      530.81     0.7743    0.8259    0.7993       270
       96.72     0.6691    0.6500    0.6594       140
       272.0     0.7556    0.6755    0.7133       151
       285.9     0.4318    0.3619    0.3938       210
       88.56     0.7628    0.8881    0.8207       134
       244.9     0.8408    0.9389    0.8871       180
         486     0.5635    0.6981    0.6236       159
       38.91     0.6154    0.1860    0.2857       129
       285.1     0.6380    0.5503    0.5909       189
       36.15     0.9648    0.9928    0.9786       138
       276.2     0.5444    0.2917    0.3798       168
         496     0.7034    0.7846    0.7418       130
       99.15     0.7209    0.5636    0.6327        55
      995.92     0.6170    0.6493    0.6327       134
      V58.61     0.7031    0.7670    0.7337       176
       507.0     0.6286    0.6947    0.6600        95
       038.9     0.5339    0.5431    0.5385       116
       88.72     0.7292    0.3646    0.4861        96
       585.9     0.7113    0.6733    0.6918       150
      403.90     0.7081    0.7037    0.7059       162
         311     0.6970    0.7709    0.7321       179
       305.1     0.6794    0.5329    0.5973       167
       37.22     0.7156    0.8125    0.7610        96
         412     0.6343    0.7589    0.6911       112
       33.24     0.7143    0.7292    0.7216        96
       39.95     0.8817    0.9318    0.9061        88
       287.5     0.5849    0.4493    0.5082       138
      410.71     0.7313    0.6901    0.7101        71
       276.1     0.6154    0.4961    0.5494       129
      V45.81     0.8544    0.9167    0.8844        96
       424.0     0.5909    0.7222    0.6500        72
       45.13     0.5806    0.8780    0.6990        82
      V15.82     0.1429    0.0191    0.0337       157
       511.9     0.3962    0.4941    0.4398        85
       37.23     0.6452    0.4167    0.5063        48

   micro avg     0.7338    0.7117    0.7226      9283
   macro avg     0.6855    0.6676    0.6674      9283
weighted avg     0.7197    0.7117    0.7087      9283
 samples avg     0.7273    0.7223    0.7016      9283

[0.863318499682136, 0.6647171010807374, 0.4664335664335664]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/llm_user/.conda/envs/torch39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

Train step of epoch 9: 100%|██████████| 1345/1345 [1:08:31<00:00,  1.89s/it, loss=0.12]
Train step of epoch 9: 100%|██████████| 1345/1345 [1:08:31<00:00,  3.06s/it, loss=0.12]

[MACRO] accuracy, precision, recall, f-measure
0.5235, 0.6845, 0.6651, 0.6746
[MICRO] accuracy, precision, recall, f-measure
0.5601, 0.7264, 0.7099, 0.7181
              precision    recall  f1-score   support

       401.9     0.7968    0.8920    0.8417       778
       38.93     0.6471    0.4652    0.5412       402
       428.0     0.8416    0.8436    0.8426       422
      427.31     0.9027    0.9277    0.9150       470
      414.01     0.8925    0.8207    0.8551       435
       96.04     0.7467    0.7210    0.7336       233
        96.6     0.6899    0.7807    0.7325       228
       584.9     0.7000    0.7348    0.7170       362
      250.00     0.7639    0.8088    0.7857       340
       96.71     0.7188    0.6240    0.6680       258
       272.4     0.7385    0.8814    0.8037       548
      518.81     0.6831    0.6510    0.6667       255
       99.04     0.1176    0.0784    0.0941        51
       39.61     0.9654    0.9867    0.9759       226
       599.0     0.7320    0.7291    0.7305       251
      530.81     0.7384    0.8383    0.7852       266
       96.72     0.7442    0.6845    0.7131       187
       272.0     0.6577    0.6323    0.6447       155
       285.9     0.4077    0.3846    0.3958       247
       88.56     0.8363    0.9108    0.8720       157
       244.9     0.8428    0.9190    0.8793       210
         486     0.6524    0.7011    0.6759       174
       38.91     0.6000    0.2027    0.3030       148
       285.1     0.6687    0.5271    0.5895       203
       36.15     0.9507    1.0000    0.9747       135
       276.2     0.6476    0.3487    0.4533       195
         496     0.6882    0.8000    0.7399       160
       99.15     0.7564    0.6484    0.6982        91
      995.92     0.6538    0.7212    0.6859       165
      V58.61     0.6137    0.7901    0.6908       181
       507.0     0.5789    0.6471    0.6111       102
       038.9     0.5786    0.5436    0.5606       149
       88.72     0.6471    0.3929    0.4889        84
       585.9     0.7013    0.6279    0.6626       172
      403.90     0.7436    0.6744    0.7073       215
         311     0.6104    0.7194    0.6604       196
       305.1     0.6503    0.5138    0.5741       181
       37.22     0.6202    0.8000    0.6987       100
         412     0.6471    0.8182    0.7226       121
       33.24     0.7545    0.6535    0.7004       127
       39.95     0.8947    0.9444    0.9189        72
       287.5     0.5280    0.4783    0.5019       138
      410.71     0.7179    0.6154    0.6627        91
       276.1     0.6827    0.4581    0.5483       155
      V45.81     0.7846    0.9444    0.8571       108
       424.0     0.7292    0.6731    0.7000       104
       45.13     0.5515    0.8621    0.6726        87
      V15.82     0.2941    0.0267    0.0490       187
       511.9     0.3985    0.5579    0.4649        95
       37.23     0.7143    0.2500    0.3704        60

   micro avg     0.7264    0.7099    0.7181     10477
   macro avg     0.6845    0.6651    0.6627     10477
weighted avg     0.7166    0.7099    0.7046     10477
 samples avg     0.7201    0.7196    0.6975     10477

[0.8727588201272412, 0.6667437825332562, 0.476229034123771]
目前最优验证集结果:0.72767

=== Epoch 9 end ===

=== Epoch 10 train ===
2023-08-17 09:12:54.704461

[MACRO] accuracy, precision, recall, f-measure
0.5304, 0.6706, 0.6814, 0.6759
[MICRO] accuracy, precision, recall, f-measure
0.5621, 0.7164, 0.7230, 0.7197
              precision    recall  f1-score   support

       401.9     0.8255    0.8955    0.8591       708
       38.93     0.5926    0.5285    0.5587       333
       428.0     0.7914    0.8220    0.8064       337
      427.31     0.9295    0.9318    0.9306       396
      414.01     0.8946    0.8531    0.8734       388
       96.04     0.6629    0.7675    0.7114       228
        96.6     0.7619    0.6400    0.6957       225
       584.9     0.6921    0.7466    0.7183       292
      250.00     0.7695    0.8201    0.7940       289
       96.71     0.7207    0.7359    0.7282       284
       272.4     0.7181    0.9011    0.7993       475
      518.81     0.6145    0.6654    0.6389       254
       99.04     0.2118    0.3333    0.2590        54
       39.61     0.9539    0.9952    0.9741       208
       599.0     0.7843    0.7477    0.7656       214
      530.81     0.7947    0.7741    0.7842       270
       96.72     0.6345    0.6571    0.6456       140
       272.0     0.7561    0.6159    0.6788       151
       285.9     0.3641    0.3190    0.3401       210
       88.56     0.7919    0.8806    0.8339       134
       244.9     0.8778    0.8778    0.8778       180
         486     0.5989    0.6855    0.6393       159
       38.91     0.5306    0.2016    0.2921       129
       285.1     0.6028    0.6825    0.6402       189
       36.15     0.9716    0.9928    0.9821       138
       276.2     0.5192    0.3214    0.3971       168
         496     0.7687    0.7923    0.7803       130
       99.15     0.6750    0.4909    0.5684        55
      995.92     0.6143    0.6418    0.6277       134
      V58.61     0.5976    0.8523    0.7026       176
       507.0     0.6559    0.6421    0.6489        95
       038.9     0.5116    0.5690    0.5388       116
       88.72     0.7551    0.3854    0.5103        96
       585.9     0.7483    0.7333    0.7407       150
      403.90     0.7421    0.7284    0.7352       162
         311     0.7049    0.7207    0.7127       179
       305.1     0.6319    0.6168    0.6242       167
       37.22     0.7455    0.8542    0.7961        96
         412     0.5986    0.7589    0.6693       112
       33.24     0.6822    0.7604    0.7192        96
       39.95     0.8889    0.9091    0.8989        88
       287.5     0.5439    0.4493    0.4921       138
      410.71     0.6623    0.7183    0.6892        71
       276.1     0.6038    0.4961    0.5447       129
      V45.81     0.8091    0.9271    0.8641        96
       424.0     0.5618    0.6944    0.6211        72
       45.13     0.6429    0.8780    0.7423        82
      V15.82     0.0000    0.0000    0.0000       157
       511.9     0.3520    0.5176    0.4190        85
       37.23     0.6667    0.5417    0.5977        48

   micro avg     0.7164    0.7230    0.7197      9283
   macro avg     0.6706    0.6814    0.6693      9283
weighted avg     0.7051    0.7230    0.7093      9283
 samples avg     0.7095    0.7315    0.6973      9283

[0.8582326764144946, 0.6619198982835347, 0.4682136045772409]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/llm_user/.conda/envs/torch39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

Train step of epoch 10: 100%|██████████| 1345/1345 [1:08:12<00:00,  1.83s/it, loss=0.109]
Train step of epoch 10: 100%|██████████| 1345/1345 [1:08:12<00:00,  3.04s/it, loss=0.109]

[MACRO] accuracy, precision, recall, f-measure
0.5229, 0.6836, 0.6731, 0.6783
[MICRO] accuracy, precision, recall, f-measure
0.5566, 0.7137, 0.7166, 0.7151
              precision    recall  f1-score   support

       401.9     0.7959    0.8972    0.8435       778
       38.93     0.6328    0.5274    0.5753       402
       428.0     0.8686    0.8460    0.8571       422
      427.31     0.9061    0.9234    0.9146       470
      414.01     0.8793    0.8207    0.8490       435
       96.04     0.7148    0.8069    0.7581       233
        96.6     0.7237    0.7237    0.7237       228
       584.9     0.6883    0.7320    0.7095       362
      250.00     0.7919    0.8059    0.7988       340
       96.71     0.6133    0.7132    0.6595       258
       272.4     0.7266    0.8777    0.7950       548
      518.81     0.6460    0.6941    0.6692       255
       99.04     0.1932    0.3333    0.2446        51
       39.61     0.9781    0.9867    0.9824       226
       599.0     0.6964    0.7769    0.7345       251
      530.81     0.7662    0.8008    0.7831       266
       96.72     0.7356    0.6845    0.7091       187
       272.0     0.6641    0.5613    0.6084       155
       285.9     0.3805    0.3482    0.3636       247
       88.56     0.8313    0.8790    0.8545       157
       244.9     0.8738    0.8571    0.8654       210
         486     0.7143    0.6322    0.6707       174
       38.91     0.6600    0.2230    0.3333       148
       285.1     0.5945    0.6355    0.6143       203
       36.15     0.9778    0.9778    0.9778       135
       276.2     0.5950    0.3692    0.4557       195
         496     0.7333    0.7562    0.7446       160
       99.15     0.7600    0.6264    0.6867        91
      995.92     0.6509    0.6667    0.6587       165
      V58.61     0.5467    0.8729    0.6723       181
       507.0     0.6154    0.6275    0.6214       102
       038.9     0.5563    0.5638    0.5600       149
       88.72     0.6047    0.3095    0.4094        84
       585.9     0.7143    0.6686    0.6907       172
      403.90     0.7833    0.7395    0.7608       215
         311     0.6215    0.6786    0.6488       196
       305.1     0.5966    0.5801    0.5882       181
       37.22     0.6154    0.8000    0.6957       100
         412     0.6178    0.8017    0.6978       121
       33.24     0.7097    0.6929    0.7012       127
       39.95     0.8933    0.9306    0.9116        72
       287.5     0.4884    0.4565    0.4719       138
      410.71     0.6364    0.6154    0.6257        91
       276.1     0.6869    0.4387    0.5354       155
      V45.81     0.7687    0.9537    0.8512       108
       424.0     0.7053    0.6442    0.6734       104
       45.13     0.5781    0.8506    0.6884        87
      V15.82     0.6667    0.0107    0.0211       187
       511.9     0.3529    0.5684    0.4355        95
       37.23     0.6286    0.3667    0.4632        60

   micro avg     0.7137    0.7166    0.7151     10477
   macro avg     0.6836    0.6731    0.6633     10477
weighted avg     0.7162    0.7166    0.7048     10477
 samples avg     0.7099    0.7260    0.6949     10477

[0.8692886061307113, 0.6646616541353384, 0.4746096009253904]
目前最优验证集结果:0.72767

=== Epoch 10 end ===

=== Epoch 11 train ===
2023-08-17 10:21:07.079972

[MACRO] accuracy, precision, recall, f-measure
0.5356, 0.6896, 0.6769, 0.6832
[MICRO] accuracy, precision, recall, f-measure
0.5671, 0.7293, 0.7183, 0.7238
              precision    recall  f1-score   support

       401.9     0.7995    0.9011    0.8473       708
       38.93     0.6000    0.5856    0.5927       333
       428.0     0.8223    0.8101    0.8161       337
      427.31     0.9340    0.9293    0.9316       396
      414.01     0.8856    0.8582    0.8717       388
       96.04     0.7021    0.7237    0.7127       228
        96.6     0.7812    0.6667    0.7194       225
       584.9     0.6928    0.7260    0.7090       292
      250.00     0.7621    0.8201    0.7900       289
       96.71     0.7209    0.7641    0.7419       284
       272.4     0.7474    0.8968    0.8153       475
      518.81     0.5778    0.7165    0.6397       254
       99.04     0.2471    0.3889    0.3022        54
       39.61     0.9495    0.9952    0.9718       208
       599.0     0.8125    0.7290    0.7685       214
      530.81     0.7826    0.8000    0.7912       270
       96.72     0.7143    0.6071    0.6564       140
       272.0     0.7317    0.5960    0.6569       151
       285.9     0.4412    0.2857    0.3468       210
       88.56     0.8000    0.8657    0.8315       134
       244.9     0.8571    0.9000    0.8780       180
         486     0.5912    0.6730    0.6294       159
       38.91     0.5952    0.1938    0.2924       129
       285.1     0.6420    0.5979    0.6192       189
       36.15     0.9648    0.9928    0.9786       138
       276.2     0.5408    0.3155    0.3985       168
         496     0.7241    0.8077    0.7636       130
       99.15     0.7045    0.5636    0.6263        55
      995.92     0.6000    0.6493    0.6237       134
      V58.61     0.6827    0.8068    0.7396       176
       507.0     0.6765    0.7263    0.7005        95
       038.9     0.5154    0.5776    0.5447       116
       88.72     0.6486    0.5000    0.5647        96
       585.9     0.7863    0.6133    0.6891       150
      403.90     0.8060    0.6667    0.7297       162
         311     0.7278    0.6872    0.7069       179
       305.1     0.6667    0.5509    0.6033       167
       37.22     0.8021    0.8021    0.8021        96
         412     0.6084    0.7768    0.6824       112
       33.24     0.7283    0.6979    0.7128        96
       39.95     0.8632    0.9318    0.8962        88
       287.5     0.4812    0.4638    0.4723       138
      410.71     0.6494    0.7042    0.6757        71
       276.1     0.5769    0.4651    0.5150       129
      V45.81     0.8700    0.9062    0.8878        96
       424.0     0.5854    0.6667    0.6234        72
       45.13     0.6635    0.8415    0.7419        82
      V15.82     0.3077    0.0255    0.0471       157
       511.9     0.4429    0.3647    0.4000        85
       37.23     0.6667    0.7083    0.6869        48

   micro avg     0.7293    0.7183    0.7238      9283
   macro avg     0.6896    0.6769    0.6749      9283
weighted avg     0.7200    0.7183    0.7123      9283
 samples avg     0.7244    0.7282    0.7020      9283

[0.8722186904005086, 0.666624284806103, 0.46656071201525745]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/llm_user/.conda/envs/torch39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

Train step of epoch 11: 100%|██████████| 1345/1345 [1:07:59<00:00,  1.97s/it, loss=0.0963]
Train step of epoch 11: 100%|██████████| 1345/1345 [1:07:59<00:00,  3.03s/it, loss=0.0963]

[MACRO] accuracy, precision, recall, f-measure
0.5247, 0.6866, 0.6657, 0.6760
[MICRO] accuracy, precision, recall, f-measure
0.5588, 0.7239, 0.7102, 0.7170
              precision    recall  f1-score   support

       401.9     0.7859    0.9010    0.8395       778
       38.93     0.6034    0.6095    0.6064       402
       428.0     0.8687    0.8152    0.8411       422
      427.31     0.9147    0.9128    0.9137       470
      414.01     0.8816    0.8391    0.8598       435
       96.04     0.7227    0.7940    0.7566       233
        96.6     0.7252    0.7061    0.7156       228
       584.9     0.7052    0.7072    0.7062       362
      250.00     0.7765    0.7971    0.7866       340
       96.71     0.6345    0.7132    0.6715       258
       272.4     0.7367    0.8832    0.8033       548
      518.81     0.6116    0.7843    0.6873       255
       99.04     0.2093    0.3529    0.2628        51
       39.61     0.9781    0.9867    0.9824       226
       599.0     0.7222    0.7251    0.7237       251
      530.81     0.7500    0.8120    0.7798       266
       96.72     0.8027    0.6310    0.7066       187
       272.0     0.6791    0.5871    0.6298       155
       285.9     0.4190    0.3036    0.3521       247
       88.56     0.8438    0.8599    0.8517       157
       244.9     0.8386    0.8905    0.8637       210
         486     0.6968    0.6207    0.6565       174
       38.91     0.5818    0.2162    0.3153       148
       285.1     0.6818    0.5911    0.6332       203
       36.15     0.9565    0.9778    0.9670       135
       276.2     0.5932    0.3590    0.4473       195
         496     0.7135    0.7625    0.7372       160
       99.15     0.7532    0.6374    0.6905        91
      995.92     0.6726    0.6848    0.6787       165
      V58.61     0.6151    0.8122    0.7000       181
       507.0     0.6075    0.6373    0.6220       102
       038.9     0.5437    0.5839    0.5631       149
       88.72     0.5938    0.4524    0.5135        84
       585.9     0.7372    0.5872    0.6537       172
      403.90     0.8249    0.6791    0.7449       215
         311     0.6335    0.6173    0.6253       196
       305.1     0.6454    0.5028    0.5652       181
       37.22     0.6566    0.6500    0.6533       100
         412     0.6275    0.7934    0.7007       121
       33.24     0.7685    0.6535    0.7064       127
       39.95     0.8734    0.9583    0.9139        72
       287.5     0.4641    0.5145    0.4880       138
      410.71     0.6818    0.6593    0.6704        91
       276.1     0.6296    0.4387    0.5171       155
      V45.81     0.7840    0.9074    0.8412       108
       424.0     0.7011    0.5865    0.6387       104
       45.13     0.6033    0.8391    0.7019        87
      V15.82     0.4167    0.0267    0.0503       187
       511.9     0.5250    0.4421    0.4800        95
       37.23     0.5370    0.4833    0.5088        60

   micro avg     0.7239    0.7102    0.7170     10477
   macro avg     0.6866    0.6657    0.6665     10477
weighted avg     0.7177    0.7102    0.7059     10477
 samples avg     0.7163    0.7208    0.6956     10477

[0.8681318681318682, 0.6643146327356854, 0.47501445922498553]
目前最优验证集结果:0.72767

=== Epoch 11 end ===

=== Epoch 12 train ===
2023-08-17 11:29:07.002308

[MACRO] accuracy, precision, recall, f-measure
0.5362, 0.6791, 0.6922, 0.6856
[MICRO] accuracy, precision, recall, f-measure
0.5649, 0.7142, 0.7299, 0.7220
              precision    recall  f1-score   support

       401.9     0.8226    0.8644    0.8430       708
       38.93     0.5665    0.6396    0.6008       333
       428.0     0.7397    0.8516    0.7917       337
      427.31     0.9163    0.9394    0.9277       396
      414.01     0.8539    0.8737    0.8637       388
       96.04     0.7069    0.7193    0.7130       228
        96.6     0.7725    0.6489    0.7053       225
       584.9     0.6851    0.7226    0.7033       292
      250.00     0.7724    0.8339    0.8020       289
       96.71     0.6994    0.7782    0.7367       284
       272.4     0.7477    0.8611    0.8004       475
      518.81     0.6754    0.6063    0.6390       254
       99.04     0.2347    0.4259    0.3026        54
       39.61     0.9581    0.9904    0.9740       208
       599.0     0.7950    0.7430    0.7681       214
      530.81     0.7619    0.8296    0.7943       270
       96.72     0.7227    0.6143    0.6641       140
       272.0     0.7398    0.6026    0.6642       151
       285.9     0.4343    0.3619    0.3948       210
       88.56     0.7905    0.8731    0.8298       134
       244.9     0.8442    0.9333    0.8865       180
         486     0.5576    0.7610    0.6436       159
       38.91     0.4884    0.3256    0.3907       129
       285.1     0.5990    0.6243    0.6114       189
       36.15     0.9580    0.9928    0.9751       138
       276.2     0.5500    0.3274    0.4104       168
         496     0.6527    0.8385    0.7340       130
       99.15     0.6735    0.6000    0.6346        55
      995.92     0.6370    0.6418    0.6394       134
      V58.61     0.6834    0.7727    0.7253       176
       507.0     0.6869    0.7158    0.7010        95
       038.9     0.5000    0.6207    0.5538       116
       88.72     0.7115    0.3854    0.5000        96
       585.9     0.6584    0.7067    0.6817       150
      403.90     0.6648    0.7469    0.7035       162
         311     0.6545    0.8045    0.7218       179
       305.1     0.6554    0.5808    0.6159       167
       37.22     0.7611    0.8958    0.8230        96
         412     0.5862    0.7589    0.6615       112
       33.24     0.6981    0.7708    0.7327        96
       39.95     0.8737    0.9432    0.9071        88
       287.5     0.5619    0.4275    0.4856       138
      410.71     0.6625    0.7465    0.7020        71
       276.1     0.6404    0.4419    0.5229       129
      V45.81     0.8713    0.9167    0.8934        96
       424.0     0.5698    0.6806    0.6203        72
       45.13     0.6273    0.8415    0.7188        82
      V15.82     0.3846    0.0637    0.1093       157
       511.9     0.4444    0.4235    0.4337        85
       37.23     0.7027    0.5417    0.6118        48

   micro avg     0.7142    0.7299    0.7220      9283
   macro avg     0.6791    0.6922    0.6774      9283
weighted avg     0.7096    0.7299    0.7136      9283
 samples avg     0.7090    0.7382    0.7008      9283

[0.8614113159567705, 0.6593769866497139, 0.4626191989828353]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/llm_user/.conda/envs/torch39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

Train step of epoch 12: 100%|██████████| 1345/1345 [1:08:29<00:00,  1.74s/it, loss=0.0845]
Train step of epoch 12: 100%|██████████| 1345/1345 [1:08:29<00:00,  3.06s/it, loss=0.0845]

[MACRO] accuracy, precision, recall, f-measure
0.5254, 0.6708, 0.6815, 0.6761
[MICRO] accuracy, precision, recall, f-measure
0.5550, 0.7058, 0.7220, 0.7138
              precision    recall  f1-score   support

       401.9     0.7974    0.8650    0.8298       778
       38.93     0.5860    0.6443    0.6137       402
       428.0     0.7935    0.8744    0.8320       422
      427.31     0.8965    0.9213    0.9087       470
      414.01     0.8469    0.8391    0.8430       435
       96.04     0.7115    0.7725    0.7407       233
        96.6     0.7149    0.7149    0.7149       228
       584.9     0.6846    0.7017    0.6930       362
      250.00     0.7577    0.8000    0.7783       340
       96.71     0.6090    0.7364    0.6667       258
       272.4     0.7464    0.8485    0.7942       548
      518.81     0.6810    0.6196    0.6489       255
       99.04     0.1743    0.3725    0.2375        51
       39.61     0.9781    0.9867    0.9824       226
       599.0     0.7096    0.7689    0.7380       251
      530.81     0.7143    0.8271    0.7666       266
       96.72     0.7919    0.6310    0.7024       187
       272.0     0.6715    0.5935    0.6301       155
       285.9     0.4106    0.3441    0.3744       247
       88.56     0.8562    0.8726    0.8644       157
       244.9     0.8348    0.9143    0.8727       210
         486     0.6173    0.6954    0.6541       174
       38.91     0.4845    0.3176    0.3837       148
       285.1     0.6218    0.5911    0.6061       203
       36.15     0.9500    0.9852    0.9673       135
       276.2     0.5868    0.3641    0.4494       195
         496     0.6480    0.7937    0.7135       160
       99.15     0.7209    0.6813    0.7006        91
      995.92     0.6724    0.7091    0.6903       165
      V58.61     0.6164    0.7901    0.6925       181
       507.0     0.6286    0.6471    0.6377       102
       038.9     0.5297    0.6577    0.5868       149
       88.72     0.6458    0.3690    0.4697        84
       585.9     0.6278    0.6570    0.6420       172
      403.90     0.7348    0.7860    0.7596       215
         311     0.5953    0.7806    0.6755       196
       305.1     0.6415    0.5635    0.6000       181
       37.22     0.6500    0.7800    0.7091       100
         412     0.6087    0.8099    0.6950       121
       33.24     0.7143    0.7087    0.7115       127
       39.95     0.8642    0.9722    0.9150        72
       287.5     0.5175    0.4275    0.4683       138
      410.71     0.6458    0.6813    0.6631        91
       276.1     0.6813    0.4000    0.5041       155
      V45.81     0.8000    0.9259    0.8584       108
       424.0     0.7396    0.6827    0.7100       104
       45.13     0.6033    0.8391    0.7019        87
      V15.82     0.3043    0.0374    0.0667       187
       511.9     0.4444    0.4211    0.4324        95
       37.23     0.6774    0.3500    0.4615        60

   micro avg     0.7058    0.7220    0.7138     10477
   macro avg     0.6708    0.6815    0.6672     10477
weighted avg     0.7001    0.7220    0.7047     10477
 samples avg     0.6992    0.7305    0.6918     10477

[0.8640832851359167, 0.6617698091382301, 0.4718912666281087]
目前最优验证集结果:0.72767

=== Epoch 12 end ===

=== Epoch 13 train ===
2023-08-17 12:37:36.431420

[MACRO] accuracy, precision, recall, f-measure
0.5294, 0.6739, 0.6774, 0.6756
[MICRO] accuracy, precision, recall, f-measure
0.5591, 0.7176, 0.7169, 0.7172
              precision    recall  f1-score   support

       401.9     0.8204    0.8644    0.8418       708
       38.93     0.5872    0.6066    0.5968       333
       428.0     0.7589    0.8220    0.7892       337
      427.31     0.9386    0.9268    0.9327       396
      414.01     0.8707    0.8505    0.8605       388
       96.04     0.7282    0.6579    0.6912       228
        96.6     0.7626    0.6711    0.7139       225
       584.9     0.6576    0.7432    0.6977       292
      250.00     0.7904    0.7958    0.7931       289
       96.71     0.7118    0.7218    0.7168       284
       272.4     0.7353    0.8716    0.7977       475
      518.81     0.6305    0.6181    0.6243       254
       99.04     0.2593    0.3889    0.3111        54
       39.61     0.9539    0.9952    0.9741       208
       599.0     0.8061    0.7383    0.7707       214
      530.81     0.7703    0.8074    0.7884       270
       96.72     0.7642    0.5786    0.6585       140
       272.0     0.7165    0.6026    0.6547       151
       285.9     0.4505    0.2381    0.3115       210
       88.56     0.7862    0.8507    0.8172       134
       244.9     0.8513    0.9222    0.8853       180
         486     0.6223    0.7358    0.6744       159
       38.91     0.5000    0.2248    0.3102       129
       285.1     0.6000    0.6349    0.6170       189
       36.15     0.9786    0.9928    0.9856       138
       276.2     0.5146    0.3155    0.3911       168
         496     0.6829    0.8615    0.7619       130
       99.15     0.6275    0.5818    0.6038        55
      995.92     0.6069    0.6567    0.6308       134
      V58.61     0.7074    0.7557    0.7308       176
       507.0     0.6604    0.7368    0.6965        95
       038.9     0.5081    0.5431    0.5250       116
       88.72     0.6104    0.4896    0.5434        96
       585.9     0.6948    0.7133    0.7039       150
      403.90     0.6936    0.7407    0.7164       162
         311     0.6380    0.7877    0.7050       179
       305.1     0.5924    0.6527    0.6211       167
       37.22     0.7069    0.8542    0.7736        96
         412     0.6028    0.7589    0.6719       112
       33.24     0.7444    0.6979    0.7204        96
       39.95     0.9101    0.9205    0.9153        88
       287.5     0.5161    0.4638    0.4885       138
      410.71     0.6420    0.7324    0.6842        71
       276.1     0.5339    0.4884    0.5101       129
      V45.81     0.8476    0.9271    0.8856        96
       424.0     0.5833    0.6806    0.6282        72
       45.13     0.7128    0.8171    0.7614        82
      V15.82     0.2553    0.0764    0.1176       157
       511.9     0.4154    0.3176    0.3600        85
       37.23     0.6364    0.4375    0.5185        48

   micro avg     0.7176    0.7169    0.7172      9283
   macro avg     0.6739    0.6774    0.6696      9283
weighted avg     0.7078    0.7169    0.7075      9283
 samples avg     0.7092    0.7264    0.6954      9283

[0.8518753973299428, 0.6591226954863318, 0.4617291799109981]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/llm_user/.conda/envs/torch39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

Train step of epoch 13: 100%|██████████| 1345/1345 [1:08:22<00:00,  1.91s/it, loss=0.0737]
Train step of epoch 13: 100%|██████████| 1345/1345 [1:08:22<00:00,  3.05s/it, loss=0.0737]

[MACRO] accuracy, precision, recall, f-measure
0.5201, 0.6760, 0.6619, 0.6689
[MICRO] accuracy, precision, recall, f-measure
0.5514, 0.7158, 0.7060, 0.7109
              precision    recall  f1-score   support

       401.9     0.7962    0.8535    0.8238       778
       38.93     0.5741    0.6070    0.5901       402
       428.0     0.8054    0.8531    0.8285       422
      427.31     0.9120    0.9043    0.9081       470
      414.01     0.8599    0.8184    0.8386       435
       96.04     0.7523    0.7167    0.7341       233
        96.6     0.7345    0.7281    0.7313       228
       584.9     0.6548    0.7127    0.6825       362
      250.00     0.7874    0.7735    0.7804       340
       96.71     0.6364    0.7054    0.6691       258
       272.4     0.7355    0.8577    0.7919       548
      518.81     0.6831    0.6510    0.6667       255
       99.04     0.1268    0.1765    0.1475        51
       39.61     0.9738    0.9867    0.9802       226
       599.0     0.7154    0.7410    0.7280       251
      530.81     0.7448    0.8120    0.7770       266
       96.72     0.7971    0.5882    0.6769       187
       272.0     0.6667    0.5935    0.6280       155
       285.9     0.4478    0.2429    0.3150       247
       88.56     0.8344    0.8344    0.8344       157
       244.9     0.8370    0.9048    0.8696       210
         486     0.6608    0.6494    0.6551       174
       38.91     0.5522    0.2500    0.3442       148
       285.1     0.6283    0.5911    0.6091       203
       36.15     0.9500    0.9852    0.9673       135
       276.2     0.5887    0.3744    0.4577       195
         496     0.6684    0.8063    0.7309       160
       99.15     0.7349    0.6703    0.7011        91
      995.92     0.6610    0.7091    0.6842       165
      V58.61     0.6432    0.7569    0.6954       181
       507.0     0.5926    0.6275    0.6095       102
       038.9     0.5845    0.5570    0.5704       149
       88.72     0.5556    0.4167    0.4762        84
       585.9     0.6705    0.6860    0.6782       172
      403.90     0.7674    0.7674    0.7674       215
         311     0.5918    0.7398    0.6576       196
       305.1     0.5846    0.6298    0.6064       181
       37.22     0.6612    0.8000    0.7240       100
         412     0.6438    0.7769    0.7041       121
       33.24     0.8119    0.6457    0.7193       127
       39.95     0.8800    0.9167    0.8980        72
       287.5     0.4962    0.4710    0.4833       138
      410.71     0.6392    0.6813    0.6596        91
       276.1     0.5528    0.4387    0.4892       155
      V45.81     0.7829    0.9352    0.8523       108
       424.0     0.7500    0.6346    0.6875       104
       45.13     0.6111    0.7586    0.6769        87
      V15.82     0.1951    0.0428    0.0702       187
       511.9     0.5352    0.4000    0.4578        95
       37.23     0.7308    0.3167    0.4419        60

   micro avg     0.7158    0.7060    0.7109     10477
   macro avg     0.6760    0.6619    0.6615     10477
weighted avg     0.7052    0.7060    0.7003     10477
 samples avg     0.7107    0.7195    0.6919     10477

[0.8582995951417004, 0.6593406593406593, 0.47044534412955463]
目前最优验证集结果:0.72767

=== Epoch 13 end ===

=== Epoch 14 train ===
2023-08-17 13:45:58.446333

[MACRO] accuracy, precision, recall, f-measure
0.5316, 0.6680, 0.6848, 0.6763
[MICRO] accuracy, precision, recall, f-measure
0.5590, 0.7095, 0.7249, 0.7171
              precision    recall  f1-score   support

       401.9     0.8048    0.8969    0.8484       708
       38.93     0.6174    0.5526    0.5832       333
       428.0     0.7294    0.8398    0.7807       337
      427.31     0.9291    0.9268    0.9279       396
      414.01     0.8597    0.8686    0.8641       388
       96.04     0.7265    0.7105    0.7184       228
        96.6     0.7017    0.7422    0.7214       225
       584.9     0.6740    0.7363    0.7038       292
      250.00     0.7538    0.8478    0.7980       289
       96.71     0.7266    0.6831    0.7042       284
       272.4     0.7500    0.8526    0.7980       475
      518.81     0.6085    0.6732    0.6393       254
       99.04     0.2308    0.3333    0.2727        54
       39.61     0.9581    0.9904    0.9740       208
       599.0     0.7725    0.7617    0.7671       214
      530.81     0.7657    0.8111    0.7878       270
       96.72     0.6691    0.6500    0.6594       140
       272.0     0.7339    0.6026    0.6618       151
       285.9     0.3864    0.3238    0.3523       210
       88.56     0.7714    0.8060    0.7883       134
       244.9     0.8737    0.9222    0.8973       180
         486     0.6190    0.7358    0.6724       159
       38.91     0.5190    0.3178    0.3942       129
       285.1     0.5980    0.6296    0.6134       189
       36.15     0.9786    0.9928    0.9856       138
       276.2     0.4497    0.3988    0.4227       168
         496     0.7181    0.8231    0.7670       130
       99.15     0.5926    0.5818    0.5872        55
      995.92     0.6099    0.6418    0.6255       134
      V58.61     0.7363    0.7614    0.7486       176
       507.0     0.6733    0.7158    0.6939        95
       038.9     0.5138    0.4828    0.4978       116
       88.72     0.6615    0.4479    0.5342        96
       585.9     0.6908    0.7000    0.6954       150
      403.90     0.7391    0.7346    0.7368       162
         311     0.6686    0.6425    0.6553       179
       305.1     0.6516    0.6048    0.6273       167
       37.22     0.7179    0.8750    0.7887        96
         412     0.5986    0.7589    0.6693       112
       33.24     0.7228    0.7604    0.7411        96
       39.95     0.9011    0.9318    0.9162        88
       287.5     0.4786    0.4855    0.4820       138
      410.71     0.6118    0.7324    0.6667        71
       276.1     0.5085    0.4651    0.4858       129
      V45.81     0.8725    0.9271    0.8990        96
       424.0     0.5814    0.6944    0.6329        72
       45.13     0.6204    0.8171    0.7053        82
      V15.82     0.2836    0.1210    0.1696       157
       511.9     0.4342    0.3882    0.4099        85
       37.23     0.6047    0.5417    0.5714        48

   micro avg     0.7095    0.7249    0.7171      9283
   macro avg     0.6680    0.6848    0.6729      9283
weighted avg     0.7024    0.7249    0.7108      9283
 samples avg     0.7043    0.7313    0.6942      9283

[0.8506039415130324, 0.6565797838525111, 0.46115702479338844]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/llm_user/.conda/envs/torch39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

Train step of epoch 14: 100%|██████████| 1345/1345 [1:08:33<00:00,  1.90s/it, loss=0.0644]
Train step of epoch 14: 100%|██████████| 1345/1345 [1:08:33<00:00,  3.06s/it, loss=0.0644]

[MACRO] accuracy, precision, recall, f-measure
0.5221, 0.6659, 0.6714, 0.6687
[MICRO] accuracy, precision, recall, f-measure
0.5512, 0.7054, 0.7161, 0.7107
              precision    recall  f1-score   support

       401.9     0.7848    0.8907    0.8344       778
       38.93     0.6037    0.5721    0.5875       402
       428.0     0.7945    0.8886    0.8389       422
      427.31     0.9064    0.9064    0.9064       470
      414.01     0.8575    0.8299    0.8435       435
       96.04     0.7322    0.7511    0.7415       233
        96.6     0.6794    0.7807    0.7265       228
       584.9     0.6813    0.7265    0.7032       362
      250.00     0.7634    0.8353    0.7978       340
       96.71     0.6520    0.6899    0.6704       258
       272.4     0.7479    0.8285    0.7861       548
      518.81     0.6387    0.6863    0.6616       255
       99.04     0.1220    0.1961    0.1504        51
       39.61     0.9824    0.9867    0.9845       226
       599.0     0.6760    0.7729    0.7212       251
      530.81     0.7239    0.8083    0.7638       266
       96.72     0.7697    0.6791    0.7216       187
       272.0     0.6667    0.5806    0.6207       155
       285.9     0.4039    0.3320    0.3644       247
       88.56     0.8243    0.7771    0.8000       157
       244.9     0.8444    0.9048    0.8736       210
         486     0.6292    0.6437    0.6364       174
       38.91     0.4944    0.2973    0.3713       148
       285.1     0.6283    0.5911    0.6091       203
       36.15     0.9638    0.9852    0.9744       135
       276.2     0.5000    0.4359    0.4658       195
         496     0.7151    0.8000    0.7552       160
       99.15     0.7229    0.6593    0.6897        91
      995.92     0.6909    0.6909    0.6909       165
      V58.61     0.6267    0.7790    0.6946       181
       507.0     0.5963    0.6373    0.6161       102
       038.9     0.5935    0.4899    0.5368       149
       88.72     0.5690    0.3929    0.4648        84
       585.9     0.6905    0.6744    0.6824       172
      403.90     0.7854    0.7488    0.7667       215
         311     0.5959    0.5867    0.5913       196
       305.1     0.6358    0.6077    0.6215       181
       37.22     0.6328    0.8100    0.7105       100
         412     0.6316    0.7934    0.7033       121
       33.24     0.7521    0.6929    0.7213       127
       39.95     0.8701    0.9306    0.8993        72
       287.5     0.4444    0.4928    0.4674       138
      410.71     0.6354    0.6703    0.6524        91
       276.1     0.5435    0.4839    0.5119       155
      V45.81     0.8033    0.9074    0.8522       108
       424.0     0.6989    0.6250    0.6599       104
       45.13     0.5667    0.7816    0.6570        87
      V15.82     0.2206    0.0802    0.1176       187
       511.9     0.4937    0.4105    0.4483        95
       37.23     0.7105    0.4500    0.5510        60

   micro avg     0.7054    0.7161    0.7107     10477
   macro avg     0.6659    0.6714    0.6643     10477
weighted avg     0.6975    0.7161    0.7037     10477
 samples avg     0.6981    0.7270    0.6890     10477

[0.8600347021399652, 0.6584152689415848, 0.4699248120300752]
目前最优验证集结果:0.72767

=== Epoch 14 end ===

=== Epoch 15 train ===
2023-08-17 14:54:32.110635

[MACRO] accuracy, precision, recall, f-measure
0.5322, 0.6704, 0.6846, 0.6774
[MICRO] accuracy, precision, recall, f-measure
0.5553, 0.7069, 0.7214, 0.7141
              precision    recall  f1-score   support

       401.9     0.8161    0.8898    0.8514       708
       38.93     0.5967    0.5465    0.5705       333
       428.0     0.7534    0.8249    0.7875       337
      427.31     0.9270    0.9293    0.9281       396
      414.01     0.8773    0.8479    0.8624       388
       96.04     0.7195    0.6974    0.7082       228
        96.6     0.7379    0.6756    0.7053       225
       584.9     0.6414    0.7534    0.6929       292
      250.00     0.7700    0.8339    0.8007       289
       96.71     0.7075    0.7324    0.7197       284
       272.4     0.7491    0.8484    0.7957       475
      518.81     0.6265    0.6142    0.6203       254
       99.04     0.2472    0.4074    0.3077        54
       39.61     0.9581    0.9904    0.9740       208
       599.0     0.7910    0.7430    0.7663       214
      530.81     0.7722    0.8037    0.7877       270
       96.72     0.7165    0.6500    0.6816       140
       272.0     0.7252    0.6291    0.6738       151
       285.9     0.3723    0.3333    0.3518       210
       88.56     0.7857    0.8209    0.8029       134
       244.9     0.8564    0.9278    0.8907       180
         486     0.5891    0.7484    0.6593       159
       38.91     0.4943    0.3333    0.3981       129
       285.1     0.5922    0.6455    0.6177       189
       36.15     0.9786    0.9928    0.9856       138
       276.2     0.4912    0.3333    0.3972       168
         496     0.6606    0.8385    0.7390       130
       99.15     0.5636    0.5636    0.5636        55
      995.92     0.5811    0.6418    0.6099       134
      V58.61     0.7000    0.7557    0.7268       176
       507.0     0.6634    0.7053    0.6837        95
       038.9     0.5000    0.5345    0.5167       116
       88.72     0.6769    0.4583    0.5466        96
       585.9     0.7143    0.7000    0.7071       150
      403.90     0.7362    0.7407    0.7385       162
         311     0.6905    0.6480    0.6686       179
       305.1     0.6597    0.5689    0.6109       167
       37.22     0.7500    0.8125    0.7800        96
         412     0.5972    0.7679    0.6719       112
       33.24     0.6822    0.7604    0.7192        96
       39.95     0.8723    0.9318    0.9011        88
       287.5     0.4627    0.4493    0.4559       138
      410.71     0.6842    0.7324    0.7075        71
       276.1     0.5876    0.4419    0.5044       129
      V45.81     0.8713    0.9167    0.8934        96
       424.0     0.6351    0.6528    0.6438        72
       45.13     0.6505    0.8171    0.7243        82
      V15.82     0.2424    0.2038    0.2215       157
       511.9     0.3646    0.4118    0.3867        85
       37.23     0.6818    0.6250    0.6522        48

   micro avg     0.7069    0.7214    0.7141      9283
   macro avg     0.6704    0.6846    0.6742      9283
weighted avg     0.7041    0.7214    0.7102      9283
 samples avg     0.7040    0.7331    0.6948      9283

[0.8518753973299428, 0.6569612205975842, 0.4600762873490146]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/llm_user/.conda/envs/torch39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

Train step of epoch 15: 100%|██████████| 1345/1345 [1:08:51<00:00,  1.87s/it, loss=0.0557]
Train step of epoch 15: 100%|██████████| 1345/1345 [1:08:51<00:00,  3.07s/it, loss=0.0557]

[MACRO] accuracy, precision, recall, f-measure
0.5214, 0.6666, 0.6693, 0.6679
[MICRO] accuracy, precision, recall, f-measure
0.5464, 0.7025, 0.7109, 0.7067
              precision    recall  f1-score   support

       401.9     0.7922    0.8869    0.8369       778
       38.93     0.6082    0.5522    0.5789       402
       428.0     0.8103    0.8602    0.8345       422
      427.31     0.9049    0.9106    0.9077       470
      414.01     0.8794    0.8046    0.8403       435
       96.04     0.7261    0.7511    0.7384       233
        96.6     0.7321    0.7193    0.7257       228
       584.9     0.6667    0.7403    0.7016       362
      250.00     0.7781    0.8147    0.7960       340
       96.71     0.6460    0.7287    0.6849       258
       272.4     0.7368    0.8175    0.7751       548
      518.81     0.6627    0.6549    0.6588       255
       99.04     0.1474    0.2745    0.1918        51
       39.61     0.9911    0.9867    0.9889       226
       599.0     0.7126    0.7410    0.7266       251
      530.81     0.7306    0.8158    0.7709       266
       96.72     0.7605    0.6791    0.7175       187
       272.0     0.6620    0.6065    0.6330       155
       285.9     0.3584    0.3279    0.3425       247
       88.56     0.8313    0.8471    0.8391       157
       244.9     0.8341    0.9095    0.8702       210
         486     0.6096    0.6552    0.6316       174
       38.91     0.4257    0.2905    0.3454       148
       285.1     0.5962    0.6256    0.6106       203
       36.15     0.9568    0.9852    0.9708       135
       276.2     0.5615    0.3744    0.4492       195
         496     0.6789    0.8063    0.7371       160
       99.15     0.7079    0.6923    0.7000        91
      995.92     0.6571    0.6970    0.6765       165
      V58.61     0.6150    0.7680    0.6830       181
       507.0     0.6058    0.6176    0.6117       102
       038.9     0.5677    0.5906    0.5789       149
       88.72     0.5667    0.4048    0.4722        84
       585.9     0.6750    0.6279    0.6506       172
      403.90     0.8187    0.7349    0.7745       215
         311     0.6089    0.6276    0.6181       196
       305.1     0.6306    0.5470    0.5858       181
       37.22     0.6396    0.7100    0.6730       100
         412     0.6419    0.7851    0.7063       121
       33.24     0.6984    0.6929    0.6957       127
       39.95     0.8734    0.9583    0.9139        72
       287.5     0.4551    0.5145    0.4830       138
      410.71     0.6951    0.6264    0.6590        91
       276.1     0.6078    0.4000    0.4825       155
      V45.81     0.7920    0.9167    0.8498       108
       424.0     0.7436    0.5577    0.6374       104
       45.13     0.6055    0.7586    0.6735        87
      V15.82     0.1805    0.1283    0.1500       187
       511.9     0.4455    0.4737    0.4592        95
       37.23     0.7000    0.4667    0.5600        60

   micro avg     0.7025    0.7109    0.7067     10477
   macro avg     0.6666    0.6693    0.6640     10477
weighted avg     0.6991    0.7109    0.7021     10477
 samples avg     0.6949    0.7216    0.6849     10477

[0.8525159051474841, 0.6589936379410064, 0.4689415847310584]
目前最优验证集结果:0.72767

=== Epoch 15 end ===

=== Epoch 16 train ===
2023-08-17 16:03:23.530510

[MACRO] accuracy, precision, recall, f-measure
0.5321, 0.6682, 0.6837, 0.6759
[MICRO] accuracy, precision, recall, f-measure
0.5551, 0.7063, 0.7216, 0.7139
              precision    recall  f1-score   support

       401.9     0.8135    0.8686    0.8402       708
       38.93     0.5757    0.5826    0.5791       333
       428.0     0.7645    0.8190    0.7908       337
      427.31     0.9311    0.9217    0.9264       396
      414.01     0.8691    0.8557    0.8623       388
       96.04     0.7243    0.6798    0.7014       228
        96.6     0.7243    0.6889    0.7062       225
       584.9     0.6697    0.7500    0.7076       292
      250.00     0.7666    0.8408    0.8020       289
       96.71     0.7000    0.7641    0.7306       284
       272.4     0.7505    0.8421    0.7937       475
      518.81     0.6125    0.6535    0.6324       254
       99.04     0.2500    0.3519    0.2923        54
       39.61     0.9581    0.9904    0.9740       208
       599.0     0.7980    0.7383    0.7670       214
      530.81     0.7806    0.8037    0.7920       270
       96.72     0.7143    0.6429    0.6767       140
       272.0     0.7174    0.6556    0.6851       151
       285.9     0.3734    0.4143    0.3928       210
       88.56     0.7887    0.8358    0.8116       134
       244.9     0.8624    0.9056    0.8835       180
         486     0.6170    0.7296    0.6686       159
       38.91     0.4639    0.3488    0.3982       129
       285.1     0.6190    0.5503    0.5826       189
       36.15     0.9716    0.9928    0.9821       138
       276.2     0.4427    0.3452    0.3880       168
         496     0.7039    0.8231    0.7589       130
       99.15     0.5636    0.5636    0.5636        55
      995.92     0.6277    0.6418    0.6347       134
      V58.61     0.7088    0.7330    0.7207       176
       507.0     0.6800    0.7158    0.6974        95
       038.9     0.5000    0.5086    0.5043       116
       88.72     0.6164    0.4688    0.5325        96
       585.9     0.7273    0.6400    0.6809       150
      403.90     0.7267    0.7222    0.7245       162
         311     0.6579    0.6983    0.6775       179
       305.1     0.6265    0.6228    0.6246       167
       37.22     0.7455    0.8542    0.7961        96
         412     0.6148    0.7411    0.6721       112
       33.24     0.7019    0.7604    0.7300        96
       39.95     0.9011    0.9318    0.9162        88
       287.5     0.4914    0.4130    0.4488       138
      410.71     0.6296    0.7183    0.6711        71
       276.1     0.5210    0.4806    0.5000       129
      V45.81     0.8812    0.9271    0.9036        96
       424.0     0.5506    0.6806    0.6087        72
       45.13     0.6600    0.8049    0.7253        82
      V15.82     0.2545    0.1783    0.2097       157
       511.9     0.3953    0.4000    0.3977        85
       37.23     0.6667    0.5833    0.6222        48

   micro avg     0.7063    0.7216    0.7139      9283
   macro avg     0.6682    0.6837    0.6738      9283
weighted avg     0.7030    0.7216    0.7106      9283
 samples avg     0.6998    0.7299    0.6912      9283

[0.8474253019707565, 0.656198347107438, 0.4588684043229498]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/llm_user/.conda/envs/torch39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

Train step of epoch 16: 100%|██████████| 1345/1345 [1:08:29<00:00,  1.97s/it, loss=0.0498]
Train step of epoch 16: 100%|██████████| 1345/1345 [1:08:29<00:00,  3.06s/it, loss=0.0498]

[MACRO] accuracy, precision, recall, f-measure
0.5203, 0.6653, 0.6660, 0.6656
[MICRO] accuracy, precision, recall, f-measure
0.5450, 0.7015, 0.7096, 0.7055
              precision    recall  f1-score   support

       401.9     0.7955    0.8702    0.8312       778
       38.93     0.5825    0.5970    0.5897       402
       428.0     0.8194    0.8602    0.8393       422
      427.31     0.9081    0.9043    0.9062       470
      414.01     0.8647    0.8230    0.8433       435
       96.04     0.7322    0.7511    0.7415       233
        96.6     0.7029    0.7368    0.7195       228
       584.9     0.6658    0.7155    0.6897       362
      250.00     0.7736    0.7941    0.7837       340
       96.71     0.6340    0.7519    0.6879       258
       272.4     0.7426    0.8212    0.7799       548
      518.81     0.6374    0.6824    0.6591       255
       99.04     0.1429    0.2157    0.1719        51
       39.61     0.9782    0.9912    0.9846       226
       599.0     0.7192    0.7450    0.7319       251
      530.81     0.7304    0.8045    0.7657       266
       96.72     0.7677    0.6364    0.6959       187
       272.0     0.6575    0.6194    0.6379       155
       285.9     0.3395    0.3725    0.3552       247
       88.56     0.8447    0.8662    0.8553       157
       244.9     0.8438    0.9000    0.8710       210
         486     0.6471    0.6322    0.6395       174
       38.91     0.4444    0.3514    0.3925       148
       285.1     0.6348    0.5567    0.5932       203
       36.15     0.9500    0.9852    0.9673       135
       276.2     0.5359    0.4205    0.4713       195
         496     0.7127    0.8063    0.7566       160
       99.15     0.7273    0.7033    0.7151        91
      995.92     0.6848    0.6848    0.6848       165
      V58.61     0.6553    0.7459    0.6977       181
       507.0     0.6095    0.6275    0.6184       102
       038.9     0.5821    0.5235    0.5512       149
       88.72     0.5429    0.4524    0.4935        84
       585.9     0.7234    0.5930    0.6518       172
      403.90     0.8095    0.7116    0.7574       215
         311     0.5841    0.6378    0.6098       196
       305.1     0.6201    0.6133    0.6167       181
       37.22     0.6167    0.7400    0.6727       100
         412     0.6331    0.7273    0.6769       121
       33.24     0.7355    0.7008    0.7177       127
       39.95     0.8608    0.9444    0.9007        72
       287.5     0.4724    0.4348    0.4528       138
      410.71     0.6867    0.6264    0.6552        91
       276.1     0.4891    0.4323    0.4589       155
      V45.81     0.8115    0.9167    0.8609       108
       424.0     0.7000    0.6058    0.6495       104
       45.13     0.6038    0.7356    0.6632        87
      V15.82     0.1402    0.0802    0.1020       187
       511.9     0.4388    0.4526    0.4456        95
       37.23     0.7273    0.4000    0.5161        60

   micro avg     0.7015    0.7096    0.7055     10477
   macro avg     0.6653    0.6660    0.6626     10477
weighted avg     0.6973    0.7096    0.7014     10477
 samples avg     0.6968    0.7191    0.6844     10477

[0.8507807981492193, 0.6572585309427414, 0.46899942163100056]
目前最优验证集结果:0.72767

=== Epoch 16 end ===

=== Epoch 17 train ===
2023-08-17 17:11:52.592056

[MACRO] accuracy, precision, recall, f-measure
0.5318, 0.6634, 0.6867, 0.6748
[MICRO] accuracy, precision, recall, f-measure
0.5551, 0.7027, 0.7255, 0.7139
              precision    recall  f1-score   support

       401.9     0.8077    0.8842    0.8442       708
       38.93     0.5597    0.5916    0.5752       333
       428.0     0.7768    0.8160    0.7959       337
      427.31     0.9175    0.9268    0.9221       396
      414.01     0.8660    0.8660    0.8660       388
       96.04     0.7093    0.7061    0.7077       228
        96.6     0.7227    0.7067    0.7146       225
       584.9     0.6707    0.7534    0.7097       292
      250.00     0.7477    0.8408    0.7915       289
       96.71     0.7181    0.7535    0.7354       284
       272.4     0.7431    0.8463    0.7913       475
      518.81     0.6265    0.6142    0.6203       254
       99.04     0.2250    0.3333    0.2687        54
       39.61     0.9581    0.9904    0.9740       208
       599.0     0.7811    0.7336    0.7566       214
      530.81     0.7742    0.8000    0.7869       270
       96.72     0.7099    0.6643    0.6863       140
       272.0     0.7143    0.6623    0.6873       151
       285.9     0.3971    0.3857    0.3913       210
       88.56     0.7872    0.8284    0.8073       134
       244.9     0.8670    0.9056    0.8859       180
         486     0.6230    0.7170    0.6667       159
       38.91     0.4194    0.4031    0.4111       129
       285.1     0.6209    0.5979    0.6092       189
       36.15     0.9786    0.9928    0.9856       138
       276.2     0.4750    0.3393    0.3958       168
         496     0.6948    0.8231    0.7535       130
       99.15     0.5536    0.5636    0.5586        55
      995.92     0.5959    0.6493    0.6214       134
      V58.61     0.7027    0.7386    0.7202       176
       507.0     0.6509    0.7263    0.6866        95
       038.9     0.4878    0.5172    0.5021       116
       88.72     0.6081    0.4688    0.5294        96
       585.9     0.7153    0.6867    0.7007       150
      403.90     0.7262    0.7531    0.7394       162
         311     0.6508    0.6872    0.6685       179
       305.1     0.6303    0.6228    0.6265       167
       37.22     0.7522    0.8854    0.8134        96
         412     0.6212    0.7321    0.6721       112
       33.24     0.7019    0.7604    0.7300        96
       39.95     0.9111    0.9318    0.9213        88
       287.5     0.4265    0.4203    0.4234       138
      410.71     0.6463    0.7465    0.6928        71
       276.1     0.5041    0.4729    0.4880       129
      V45.81     0.8812    0.9271    0.9036        96
       424.0     0.5376    0.6944    0.6061        72
       45.13     0.6562    0.7683    0.7079        82
      V15.82     0.2347    0.1465    0.1804       157
       511.9     0.4167    0.4118    0.4142        85
       37.23     0.6667    0.5417    0.5977        48

   micro avg     0.7027    0.7255    0.7139      9283
   macro avg     0.6634    0.6867    0.6729      9283
weighted avg     0.6986    0.7255    0.7104      9283
 samples avg     0.6975    0.7326    0.6912      9283

[0.8480610298792117, 0.6568340750158932, 0.45944055944055945]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/llm_user/.conda/envs/torch39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

Train step of epoch 17: 100%|██████████| 1345/1345 [1:08:40<00:00,  1.98s/it, loss=0.045]
Train step of epoch 17: 100%|██████████| 1345/1345 [1:08:40<00:00,  3.06s/it, loss=0.045]

[MACRO] accuracy, precision, recall, f-measure
0.5219, 0.6622, 0.6718, 0.6670
[MICRO] accuracy, precision, recall, f-measure
0.5468, 0.6987, 0.7155, 0.7070
              precision    recall  f1-score   support

       401.9     0.7938    0.8856    0.8372       778
       38.93     0.5758    0.6144    0.5945       402
       428.0     0.8211    0.8483    0.8345       422
      427.31     0.9017    0.9170    0.9093       470
      414.01     0.8657    0.8299    0.8474       435
       96.04     0.7490    0.7682    0.7585       233
        96.6     0.7008    0.7500    0.7246       228
       584.9     0.6641    0.7210    0.6914       362
      250.00     0.7603    0.8118    0.7852       340
       96.71     0.6485    0.7364    0.6897       258
       272.4     0.7318    0.8266    0.7763       548
      518.81     0.6452    0.6275    0.6362       255
       99.04     0.1395    0.2353    0.1752        51
       39.61     0.9738    0.9867    0.9802       226
       599.0     0.6974    0.7530    0.7241       251
      530.81     0.7186    0.7970    0.7558       266
       96.72     0.7758    0.6845    0.7273       187
       272.0     0.6690    0.6129    0.6397       155
       285.9     0.3548    0.3563    0.3556       247
       88.56     0.8408    0.8408    0.8408       157
       244.9     0.8400    0.9000    0.8690       210
         486     0.6488    0.6264    0.6374       174
       38.91     0.3986    0.3851    0.3918       148
       285.1     0.6330    0.5862    0.6087       203
       36.15     0.9638    0.9852    0.9744       135
       276.2     0.5586    0.4154    0.4765       195
         496     0.7127    0.8063    0.7566       160
       99.15     0.7326    0.6923    0.7119        91
      995.92     0.6723    0.7212    0.6959       165
      V58.61     0.6402    0.7569    0.6937       181
       507.0     0.6239    0.6667    0.6445       102
       038.9     0.5563    0.5638    0.5600       149
       88.72     0.5139    0.4405    0.4744        84
       585.9     0.7013    0.6279    0.6626       172
      403.90     0.8020    0.7349    0.7670       215
         311     0.5762    0.6173    0.5961       196
       305.1     0.6163    0.5856    0.6006       181
       37.22     0.6134    0.7300    0.6667       100
         412     0.6414    0.7686    0.6992       121
       33.24     0.7295    0.7008    0.7149       127
       39.95     0.8734    0.9583    0.9139        72
       287.5     0.4276    0.4710    0.4483       138
      410.71     0.6667    0.6593    0.6630        91
       276.1     0.5037    0.4387    0.4690       155
      V45.81     0.8151    0.8981    0.8546       108
       424.0     0.7021    0.6346    0.6667       104
       45.13     0.5905    0.7126    0.6458        87
      V15.82     0.1798    0.0856    0.1159       187
       511.9     0.4388    0.4526    0.4456        95
       37.23     0.7097    0.3667    0.4835        60

   micro avg     0.6987    0.7155    0.7070     10477
   macro avg     0.6622    0.6718    0.6638     10477
weighted avg     0.6946    0.7155    0.7029     10477
 samples avg     0.6950    0.7244    0.6863     10477

[0.8455754771544245, 0.6576055523423945, 0.46905725853094277]
目前最优验证集结果:0.72767

=== Epoch 17 end ===

=== Epoch 18 train ===
2023-08-17 18:20:32.811804

[MACRO] accuracy, precision, recall, f-measure
0.5328, 0.6665, 0.6850, 0.6756
[MICRO] accuracy, precision, recall, f-measure
0.5554, 0.7060, 0.7224, 0.7141
              precision    recall  f1-score   support

       401.9     0.8152    0.8785    0.8457       708
       38.93     0.5655    0.5706    0.5680       333
       428.0     0.7561    0.8279    0.7904       337
      427.31     0.9242    0.9242    0.9242       396
      414.01     0.8653    0.8608    0.8630       388
       96.04     0.7078    0.6798    0.6935       228
        96.6     0.7407    0.7111    0.7256       225
       584.9     0.6837    0.7329    0.7074       292
      250.00     0.7742    0.8304    0.8013       289
       96.71     0.7260    0.7465    0.7361       284
       272.4     0.7519    0.8358    0.7916       475
      518.81     0.6103    0.6535    0.6312       254
       99.04     0.2237    0.3148    0.2615        54
       39.61     0.9581    0.9904    0.9740       208
       599.0     0.7861    0.7383    0.7614       214
      530.81     0.7742    0.8000    0.7869       270
       96.72     0.7121    0.6714    0.6912       140
       272.0     0.7239    0.6424    0.6807       151
       285.9     0.3762    0.3762    0.3762       210
       88.56     0.7887    0.8358    0.8116       134
       244.9     0.8624    0.9056    0.8835       180
         486     0.6333    0.7170    0.6726       159
       38.91     0.4310    0.3876    0.4082       129
       285.1     0.6138    0.6138    0.6138       189
       36.15     0.9786    0.9928    0.9856       138
       276.2     0.4758    0.3512    0.4041       168
         496     0.6908    0.8077    0.7447       130
       99.15     0.5614    0.5818    0.5714        55
      995.92     0.6159    0.6343    0.6250       134
      V58.61     0.7017    0.7216    0.7115       176
       507.0     0.6491    0.7789    0.7081        95
       038.9     0.5085    0.5172    0.5128       116
       88.72     0.6197    0.4583    0.5269        96
       585.9     0.7214    0.6733    0.6966       150
      403.90     0.7500    0.7593    0.7546       162
         311     0.6541    0.6760    0.6648       179
       305.1     0.6415    0.6108    0.6258       167
       37.22     0.7500    0.8750    0.8077        96
         412     0.6165    0.7321    0.6694       112
       33.24     0.7019    0.7604    0.7300        96
       39.95     0.9111    0.9318    0.9213        88
       287.5     0.4297    0.3986    0.4135       138
      410.71     0.6667    0.7324    0.6980        71
       276.1     0.5042    0.4651    0.4839       129
      V45.81     0.8812    0.9271    0.9036        96
       424.0     0.5319    0.6944    0.6024        72
       45.13     0.6500    0.7927    0.7143        82
      V15.82     0.2315    0.1592    0.1887       157
       511.9     0.4167    0.4118    0.4142        85
       37.23     0.6585    0.5625    0.6067        48

   micro avg     0.7060    0.7224    0.7141      9283
   macro avg     0.6665    0.6850    0.6737      9283
weighted avg     0.7021    0.7224    0.7108      9283
 samples avg     0.6998    0.7294    0.6906      9283

[0.8461538461538461, 0.6572155117609663, 0.4592498410680229]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/llm_user/.conda/envs/torch39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

Train step of epoch 18: 100%|██████████| 1345/1345 [1:08:14<00:00,  1.70s/it, loss=0.0418]
Train step of epoch 18: 100%|██████████| 1345/1345 [1:08:14<00:00,  3.04s/it, loss=0.0418]

[MACRO] accuracy, precision, recall, f-measure
0.5204, 0.6641, 0.6666, 0.6654
[MICRO] accuracy, precision, recall, f-measure
0.5446, 0.7005, 0.7098, 0.7052
              precision    recall  f1-score   support

       401.9     0.7981    0.8843    0.8390       778
       38.93     0.5809    0.5896    0.5852       402
       428.0     0.8085    0.8602    0.8335       422
      427.31     0.9049    0.9106    0.9077       470
      414.01     0.8633    0.8276    0.8451       435
       96.04     0.7415    0.7511    0.7463       233
        96.6     0.7025    0.7456    0.7234       228
       584.9     0.6757    0.6906    0.6831       362
      250.00     0.7839    0.8000    0.7918       340
       96.71     0.6448    0.7248    0.6825       258
       272.4     0.7420    0.8084    0.7738       548
      518.81     0.6377    0.6902    0.6629       255
       99.04     0.1250    0.1961    0.1527        51
       39.61     0.9737    0.9823    0.9780       226
       599.0     0.7094    0.7490    0.7287       251
      530.81     0.7279    0.8045    0.7643       266
       96.72     0.7764    0.6684    0.7184       187
       272.0     0.6691    0.6000    0.6327       155
       285.9     0.3398    0.3563    0.3478       247
       88.56     0.8354    0.8408    0.8381       157
       244.9     0.8475    0.9000    0.8730       210
         486     0.6463    0.6092    0.6272       174
       38.91     0.4044    0.3716    0.3873       148
       285.1     0.6387    0.6010    0.6193       203
       36.15     0.9568    0.9852    0.9708       135
       276.2     0.5359    0.4205    0.4713       195
         496     0.7209    0.7750    0.7470       160
       99.15     0.7283    0.7363    0.7322        91
      995.92     0.6829    0.6788    0.6809       165
      V58.61     0.6490    0.7459    0.6941       181
       507.0     0.6053    0.6765    0.6389       102
       038.9     0.5821    0.5235    0.5512       149
       88.72     0.5362    0.4405    0.4837        84
       585.9     0.7095    0.6105    0.6562       172
      403.90     0.8105    0.7163    0.7605       215
         311     0.5648    0.6224    0.5922       196
       305.1     0.6341    0.5746    0.6029       181
       37.22     0.6207    0.7200    0.6667       100
         412     0.6503    0.7686    0.7045       121
       33.24     0.7273    0.6929    0.7097       127
       39.95     0.8718    0.9444    0.9067        72
       287.5     0.4267    0.4638    0.4444       138
      410.71     0.6988    0.6374    0.6667        91
       276.1     0.4924    0.4194    0.4530       155
      V45.81     0.8167    0.9074    0.8596       108
       424.0     0.6915    0.6250    0.6566       104
       45.13     0.5943    0.7241    0.6528        87
      V15.82     0.1892    0.1123    0.1409       187
       511.9     0.4356    0.4632    0.4490        95
       37.23     0.6970    0.3833    0.4946        60

   micro avg     0.7005    0.7098    0.7052     10477
   macro avg     0.6641    0.6666    0.6626     10477
weighted avg     0.6971    0.7098    0.7017     10477
 samples avg     0.6966    0.7195    0.6845     10477

[0.8519375361480624, 0.6569115095430885, 0.46778484673221515]
目前最优验证集结果:0.72767

=== Epoch 18 end ===

=== Epoch 19 train ===
2023-08-17 19:28:47.715632

[MACRO] accuracy, precision, recall, f-measure
0.5322, 0.6673, 0.6828, 0.6750
[MICRO] accuracy, precision, recall, f-measure
0.5547, 0.7069, 0.7203, 0.7136
              precision    recall  f1-score   support

       401.9     0.8152    0.8785    0.8457       708
       38.93     0.5659    0.5676    0.5667       333
       428.0     0.7582    0.8279    0.7915       337
      427.31     0.9264    0.9217    0.9241       396
      414.01     0.8668    0.8557    0.8612       388
       96.04     0.7110    0.6798    0.6951       228
        96.6     0.7395    0.7067    0.7227       225
       584.9     0.6815    0.7329    0.7063       292
      250.00     0.7732    0.8374    0.8040       289
       96.71     0.7182    0.7359    0.7270       284
       272.4     0.7514    0.8337    0.7904       475
      518.81     0.6240    0.6339    0.6289       254
       99.04     0.2237    0.3148    0.2615        54
       39.61     0.9581    0.9904    0.9740       208
       599.0     0.7811    0.7336    0.7566       214
      530.81     0.7782    0.7926    0.7853       270
       96.72     0.7121    0.6714    0.6912       140
       272.0     0.7273    0.6358    0.6784       151
       285.9     0.3803    0.3857    0.3830       210
       88.56     0.7887    0.8358    0.8116       134
       244.9     0.8670    0.9056    0.8859       180
         486     0.6250    0.7233    0.6706       159
       38.91     0.4310    0.3876    0.4082       129
       285.1     0.6126    0.6190    0.6158       189
       36.15     0.9786    0.9928    0.9856       138
       276.2     0.4758    0.3512    0.4041       168
         496     0.6863    0.8077    0.7420       130
       99.15     0.5536    0.5636    0.5586        55
      995.92     0.6099    0.6418    0.6255       134
      V58.61     0.7095    0.7216    0.7155       176
       507.0     0.6604    0.7368    0.6965        95
       038.9     0.5042    0.5172    0.5106       116
       88.72     0.6197    0.4583    0.5269        96
       585.9     0.7353    0.6667    0.6993       150
      403.90     0.7546    0.7593    0.7569       162
         311     0.6593    0.6704    0.6648       179
       305.1     0.6335    0.6108    0.6220       167
       37.22     0.7500    0.8750    0.8077        96
         412     0.6212    0.7321    0.6721       112
       33.24     0.7019    0.7604    0.7300        96
       39.95     0.9111    0.9318    0.9213        88
       287.5     0.4400    0.3986    0.4183       138
      410.71     0.6667    0.7324    0.6980        71
       276.1     0.5042    0.4651    0.4839       129
      V45.81     0.8812    0.9271    0.9036        96
       424.0     0.5376    0.6944    0.6061        72
       45.13     0.6633    0.7927    0.7222        82
      V15.82     0.2264    0.1529    0.1825       157
       511.9     0.4070    0.4118    0.4094        85
       37.23     0.6585    0.5625    0.6067        48

   micro avg     0.7069    0.7203    0.7136      9283
   macro avg     0.6673    0.6828    0.6731      9283
weighted avg     0.7029    0.7203    0.7102      9283
 samples avg     0.7006    0.7280    0.6904      9283

[0.8448823903369358, 0.6563254926891291, 0.45874125874125876]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/llm_user/.conda/envs/torch39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

Train step of epoch 19: 100%|██████████| 1345/1345 [1:08:07<00:00,  1.70s/it, loss=0.0406]
Train step of epoch 19: 100%|██████████| 1345/1345 [1:08:07<00:00,  3.04s/it, loss=0.0406]

[MACRO] accuracy, precision, recall, f-measure
0.5196, 0.6650, 0.6641, 0.6645
[MICRO] accuracy, precision, recall, f-measure
0.5444, 0.7023, 0.7077, 0.7050
              precision    recall  f1-score   support

       401.9     0.7974    0.8805    0.8369       778
       38.93     0.5877    0.5920    0.5898       402
       428.0     0.8103    0.8602    0.8345       422
      427.31     0.9085    0.9085    0.9085       470
      414.01     0.8671    0.8253    0.8457       435
       96.04     0.7384    0.7511    0.7447       233
        96.6     0.7071    0.7412    0.7238       228
       584.9     0.6765    0.6934    0.6849       362
      250.00     0.7845    0.8029    0.7936       340
       96.71     0.6448    0.7248    0.6825       258
       272.4     0.7416    0.8066    0.7727       548
      518.81     0.6353    0.6627    0.6488       255
       99.04     0.1266    0.1961    0.1538        51
       39.61     0.9737    0.9823    0.9780       226
       599.0     0.7165    0.7450    0.7305       251
      530.81     0.7285    0.7970    0.7612       266
       96.72     0.7778    0.6738    0.7221       187
       272.0     0.6739    0.6000    0.6348       155
       285.9     0.3398    0.3522    0.3459       247
       88.56     0.8302    0.8408    0.8354       157
       244.9     0.8475    0.9000    0.8730       210
         486     0.6391    0.6207    0.6297       174
       38.91     0.4074    0.3716    0.3887       148
       285.1     0.6354    0.6010    0.6177       203
       36.15     0.9568    0.9852    0.9708       135
       276.2     0.5395    0.4205    0.4726       195
         496     0.7209    0.7750    0.7470       160
       99.15     0.7253    0.7253    0.7253        91
      995.92     0.6807    0.6848    0.6828       165
      V58.61     0.6522    0.7459    0.6959       181
       507.0     0.6161    0.6765    0.6449       102
       038.9     0.5896    0.5302    0.5583       149
       88.72     0.5373    0.4286    0.4768        84
       585.9     0.7095    0.6105    0.6562       172
      403.90     0.8148    0.7163    0.7624       215
         311     0.5735    0.6173    0.5946       196
       305.1     0.6319    0.5691    0.5988       181
       37.22     0.6316    0.7200    0.6729       100
         412     0.6429    0.7438    0.6897       121
       33.24     0.7333    0.6929    0.7126       127
       39.95     0.8718    0.9444    0.9067        72
       287.5     0.4306    0.4493    0.4397       138
      410.71     0.6824    0.6374    0.6591        91
       276.1     0.4924    0.4194    0.4530       155
      V45.81     0.8167    0.9074    0.8596       108
       424.0     0.6957    0.6154    0.6531       104
       45.13     0.6000    0.7241    0.6562        87
      V15.82     0.1852    0.1070    0.1356       187
       511.9     0.4356    0.4632    0.4490        95
       37.23     0.6875    0.3667    0.4783        60

   micro avg     0.7023    0.7077    0.7050     10477
   macro avg     0.6650    0.6641    0.6618     10477
weighted avg     0.6983    0.7077    0.7013     10477
 samples avg     0.6981    0.7177    0.6846     10477

[0.8478889531521111, 0.6562174667437826, 0.46790052053209946]
目前最优验证集结果:0.72767

=== Epoch 19 end ===
==============Finish==============
