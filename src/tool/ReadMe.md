## 正反义词测试工具

### 简介
在机器学习中，识别和理解反义词的定义存在一些困难。这主要是因为反义词之间的关系通常是相对的，取决于上下文和语境。以下是一些可能导致难以识别反义词定义的原因：

语义多样性：同一个单词可能具有多个含义，根据上下文不同，它可以作为反义词出现，这增加了对其进行准确定义的挑战。例如，“快”在某些情况下是正面含义，表示速度快，而在其他情况下是负面含义，表示时间紧迫。

上下文依赖性：确定反义词需要考虑整个句子或段落的上下文。同一个单词在不同的上下文中可能具有相反的含义。因此，仅仅依靠单个词汇特征来判断是否为反义词是不够的。

数据限制：训练机器学习模型需要大量的数据样本来学习反义词之间的关系。然而，在构建反义词对的标记数据方面可能存在困难。由于反义词的概念相对主观，人们可能对某些词语的反义词产生争议，这导致构建准确的标记数据集变得具有挑战性。

语言变化：语言是一种动态的媒介，词汇的含义和用法会随着时间的推移而变化。机器学习模型在训练时往往基于特定时间段的数据，这可能导致对于最新的反义词定义无法准确识别。

因此，目前仍无法找到一种通用的评判体系用于机器学习上准确地识别各领域的反义词对。

本工具通过预设词典+word2vec向量化+分词，尽可能拓展词语涵义关联覆盖面，从而提供一般范围下正反义判断的通用性。
### 环境依赖
<pre><code>
pip install -r requirements.txt
</code></pre>
python版本：3.6.8

### 下载相关模型
链接：https://pan.baidu.com/s/14K6UxSxETB_L1PtU-3Z4Bw?pwd=h72k 

提取码：h72k

将wiki_zh.model.wv.vectors.npy和wiki_zh.model.syn1neg.npy放到目录文件夹下

### 快速开始
将指定格式的 csv 文件放入data文件夹中，进入test_model.py文件main函数

<pre><code>
if __name__ == '__main__':
    k = Actor() # 执行器
    k.csv_check("demo.csv") # 修改为文件名
</code></pre>

运行后等待输出即可

### 预设词典文件
antisem.txt 文件为词典文件，格式为反义词一对多，后续若需要添加高频词组则follow已有格式即可

### 处理逻辑
1. 读取csv文件，取出原词、给定近义词、给定反义词。
2. 扫描预设词典，若原词位于词典中，则直接返回预设结果；否则进入word2vec模型处理流程。---级别1
3. 通过中文维基百科训练word2vec nlp处理模型，通过模型获取与原词最相近的前n个词语，将该n个词语输入词典扫描，若命中预设结果，则返回结果；否则进入分词环节。 ---级别2
4. 对原词进行jieba分词处理，如“电信网络诈骗”分为“电信”“网络诈骗“，分词所得词汇进行词典扫描，若命中结果，则将结果累计于返回结果中；否则，使用word2vec对上述分词进行相近词语查找，再扫描相关词典。---级别3
5. 对csv中的近义词、反义词与上述结果进行包含匹配，若给定词中含有结果集元素，则输出命中近义/反义判断。
6. 对所有词语的命中与总数进行求商，得到近义词、反义词准确率。

### 使用建议与改进
本工具目前对预设词典依赖性较大，若后续评估后认为有较大必要对近义、反义识别进行精度提升，有以下改进建议：
1. 不断深化词典的专业程度，通过人力小范围标注预设高频领域词汇，从而产生大范围的识别效果。
2. 引入分类器模型，人工制作数据集进行训练，接入本工具，既实现词语匹配，也实现预测。
3. 对word2vec等类似nlp模型进行特定领域的精细训练。

### Word2Vec
词向量转换是指对大批文本进行潜在关联学习，在学习训练得到的规则下利用向量表示每个特定词语与词语之间的语义距离。词向量的形式量化了本来模糊的词语边界，有利于计算机对词语进行后续处理，能够更加轻松的预测上下文。Word2Vec是谷歌于2013年提出的典型自然语言处理深度模型，由适应小样本的CBOW模型和适合大型语料库的Skip-gram模型组成。输入是经过One-hot编码的词语向量矩阵，隐藏层由线性神经元组成，输出层输出维度与输入层一致，采用SoftMax的方式归一化[66]。根据开源项目信息的特点，本课题选用基于Skip-gram的Word2Vec进行词语向量化。Skip-gram模型处理词语的原理是首先定义概率分布概念，在一定大小的滑动窗口内，用于描述该词上下文词语出现的概率；其次构建One-hot编码后的中心词向量矩阵与上下文词向量矩阵；将对应中心词向量与上下文词向量矩阵进行内积运算与Softmax归一化得到一行多列矩阵，每列的内积结果越大，则表示该位置的词语与中心词的相关性越强；调整中心词矩阵与上下文矩阵，构造损失函数使用反向传播策略对参数进行动态调整，最终实现损失函数最小化。Word2Vec使用了一种称为“分布式假设”的思想，即相似意思的单词在上下文中通常出现在类似的环境中。例如，在一个句子中，像"apple"和"orange"这样的水果词可能会在相似的上下文中出现，比如与"eat"或"juice"等词一起出现。

利用Word2Vec生成的词向量，我们可以进行各种自然语言处理任务。例如，可以计算两个单词之间的相似度，通过测量它们在向量空间中的距离来衡量它们的语义相似性。还可以使用这些向量进行文本分类、情感分析、信息检索等任务。

在本工具中，Word2Vec被用于延申词汇同义词，以向量最相近的若干个词语指替目标词语，从而进行拓展匹配，补全词典覆盖面小的缺点。

#### 使用效果
对于通用性较强的词语，效果不错，如：
![在这里插入图片描述](https://img-blog.csdnimg.cn/40959df158a748d08066e75bbf31dbb4.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/169e33066c40460385afc4687ff03270.png)
对于领域性较强的词语，由于没有进行词典的补充与单独训练，效果欠佳，如：
![在这里插入图片描述](https://img-blog.csdnimg.cn/a90d67560ea04212b2015dda2e5486f8.png)
因此，使用者应结合实际情况对项目文件进行修改适配。